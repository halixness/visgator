{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Gzfov9UKpGR"
      },
      "source": [
        "## Referring Expression Comprehension as Scene Graph Grounding\n",
        "\n",
        "### Authors\n",
        "\n",
        "Diego Calanzone, Francesco Gentile <br>\n",
        "University of Trento <br>\n",
        "Deep Learning course project, Spring 2023"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notes on the code\n",
        "\n",
        "For this project we have decided to develop a complete framework that can be easily extended to many computer vision tasks by simply defining the necessary metrics and inputs and targets. This made extremely easy to test different models without reuqiring any modifications to parts of the code not related to the model.\n",
        "\n",
        "In this notebook, we do not report all the infrastructure since it is not the relevant part of the project. We only report those code snippets that are associated to our proposed model. The complete framework can be found at https://github.com/FrancescoGentile/visgator in the `deepsight` branch.\n",
        "\n",
        "The structure of the project is highly inspired on that of [detrex](https://github.com/IDEA-Research/detrex) and is:\n",
        "\n",
        "```\n",
        "datasets/ # here you can find all the datasets (at the moment only RefCOCOg)\n",
        "deepsight/ # this contain the framework source code\n",
        "  data/\n",
        "    structs/ # dataclasses used to model the input and output of different tasks\n",
        "    datasets/ # interface to be implemented bt all datasets\n",
        "    transformations/ # data aumentation transforms\n",
        "  engines/ # contains the trainer and tester\n",
        "  modeling/\n",
        "    layers/ # reusable layers\n",
        "    detectors/ # wrappers around available object detectors\n",
        "    parsers/ # modules to extract scene graphs from sentences\n",
        "    pipeline/ # contains the interfaces for the creation of new models\n",
        "  measures/ # contains losses and metrics\n",
        "  optimizers/\n",
        "  lr_schedulers/\n",
        "  utils/\n",
        "projects/ # contains the implementation of the proposed model and the baseline\n",
        "```"
      ],
      "metadata": {
        "id": "4SxncKBC3Ssb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preliminaries"
      ],
      "metadata": {
        "id": "x2856Wr0XSbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/FrancescoGentile/visgator\n",
        "%cd visgator\n",
        "!git checkout deepsight"
      ],
      "metadata": {
        "id": "dNKBdHVegX7A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ef1097b-5a17-4399-a054-cded28c0dda8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'visgator'...\n",
            "remote: Enumerating objects: 1264, done.\u001b[K\n",
            "remote: Counting objects: 100% (434/434), done.\u001b[K\n",
            "remote: Compressing objects: 100% (277/277), done.\u001b[K\n",
            "remote: Total 1264 (delta 153), reused 377 (delta 128), pack-reused 830\u001b[K\n",
            "Receiving objects: 100% (1264/1264), 1.90 MiB | 24.31 MiB/s, done.\n",
            "Resolving deltas: 100% (655/655), done.\n",
            "/content/visgator\n",
            "Branch 'deepsight' set up to track remote branch 'deepsight' from 'origin'.\n",
            "Switched to a new branch 'deepsight'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and extract the RefCOCOg dataset\n",
        "!pip install -q gdown\n",
        "!gdown 1hxk0f62WtczYGp_zMBE_SQuqfsUvSNld\n",
        "!apt-get install unrar\n",
        "!unrar x refcocog.rar"
      ],
      "metadata": {
        "id": "N91QNsUpXXXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1HA0GbP0MXMm8UOC-ties9DtFoI_NUt9l"
      ],
      "metadata": {
        "id": "7FOX32HMl5CH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm refcocog.rar\n",
        "!mkdir data\n",
        "!mv refcocog data\n",
        "!mv scene_graphs.json data/refcocog/annotations"
      ],
      "metadata": {
        "id": "flToGuQioyH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch==2.0.1 torchvision==0.15.2\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "!pip install -q jaxtyping torchmetrics pyserde numpy rustworkx transformers scikit-learn ruamel-yaml wandb albumentations openai ultralytics\n",
        "!pip install -q SceneGraphParser diaparser\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "A_pBSePrggVj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "048c16c4-8305-4061-8a28-edf55b025fc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m728.8/728.8 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m627.5/627.5 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.2/213.2 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h2023-07-11 20:29:02.536511: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-11 20:29:04.353690: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-07-11 20:29:06.634046: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-11 20:29:06.636650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-11 20:29:06.636845: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.11)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "id": "gLJ-NPuehndh",
        "outputId": "0b39ccec-4182-4330-8dfc-5a56038f7df2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/visgator\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from deepsight==0.1.0+editable) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from deepsight==0.1.0+editable) (0.15.2+cu118)\n",
            "Requirement already satisfied: jaxtyping>=0.2.20 in /usr/local/lib/python3.10/dist-packages (from deepsight==0.1.0+editable) (0.2.20)\n",
            "Requirement already satisfied: torchmetrics>=0.11.4 in /usr/local/lib/python3.10/dist-packages (from deepsight==0.1.0+editable) (1.0.0)\n",
            "Requirement already satisfied: pyserde>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from deepsight==0.1.0+editable) (0.11.1)\n",
            "Collecting numpy>=1.25.0 (from deepsight==0.1.0+editable)\n",
            "  Downloading numpy-1.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rustworkx>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from deepsight==0.1.0+editable) (0.13.0)\n",
            "Requirement already satisfied: transformers>=4.30.2 in /usr/local/lib/python3.10/dist-packages (from deepsight==0.1.0+editable) (4.30.2)\n",
            "Collecting scikit-learn>=1.3.0 (from deepsight==0.1.0+editable)\n",
            "  Downloading scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ruamel-yaml>=0.17.32 in /usr/local/lib/python3.10/dist-packages (from deepsight==0.1.0+editable) (0.17.32)\n",
            "Requirement already satisfied: wandb>=0.15.4 in /usr/local/lib/python3.10/dist-packages (from deepsight==0.1.0+editable) (0.15.5)\n",
            "Collecting albumentations>=1.3.1 (from deepsight==0.1.0+editable)\n",
            "  Downloading albumentations-1.3.1-py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.7/125.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openai>=0.27.8 in /usr/local/lib/python3.10/dist-packages (from deepsight==0.1.0+editable) (0.27.8)\n",
            "Requirement already satisfied: ultralytics>=8.0.131 in /usr/local/lib/python3.10/dist-packages (from deepsight==0.1.0+editable) (8.0.132)\n",
            "Requirement already satisfied: torch-scatter>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from deepsight==0.1.0+editable) (2.1.1+pt20cu118)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from albumentations>=1.3.1->deepsight==0.1.0+editable) (1.10.1)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations>=1.3.1->deepsight==0.1.0+editable) (0.19.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations>=1.3.1->deepsight==0.1.0+editable) (6.0)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations>=1.3.1->deepsight==0.1.0+editable) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations>=1.3.1->deepsight==0.1.0+editable) (4.8.0.74)\n",
            "Requirement already satisfied: typeguard>=2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.20->deepsight==0.1.0+editable) (4.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.20->deepsight==0.1.0+editable) (4.7.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai>=0.27.8->deepsight==0.1.0+editable) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai>=0.27.8->deepsight==0.1.0+editable) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai>=0.27.8->deepsight==0.1.0+editable) (3.8.4)\n",
            "Requirement already satisfied: casefy in /usr/local/lib/python3.10/dist-packages (from pyserde>=0.11.0->deepsight==0.1.0+editable) (0.1.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyserde>=0.11.0->deepsight==0.1.0+editable) (3.1.2)\n",
            "Requirement already satisfied: typing_inspect>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pyserde>=0.11.0->deepsight==0.1.0+editable) (0.9.0)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from ruamel-yaml>=0.17.32->deepsight==0.1.0+editable) (0.2.7)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->deepsight==0.1.0+editable) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->deepsight==0.1.0+editable) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->deepsight==0.1.0+editable) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->deepsight==0.1.0+editable) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->deepsight==0.1.0+editable) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->deepsight==0.1.0+editable) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=2.0.1->deepsight==0.1.0+editable) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=2.0.1->deepsight==0.1.0+editable) (16.0.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.11.4->deepsight==0.1.0+editable) (23.1)\n",
            "Requirement already satisfied: lightning-utilities>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.11.4->deepsight==0.1.0+editable) (0.9.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.15.2->deepsight==0.1.0+editable) (8.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.2->deepsight==0.1.0+editable) (0.16.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.2->deepsight==0.1.0+editable) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.2->deepsight==0.1.0+editable) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.2->deepsight==0.1.0+editable) (0.3.1)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.0.131->deepsight==0.1.0+editable) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.0.131->deepsight==0.1.0+editable) (4.7.0.72)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.0.131->deepsight==0.1.0+editable) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.0.131->deepsight==0.1.0+editable) (0.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.0.131->deepsight==0.1.0+editable) (5.9.5)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.15.4->deepsight==0.1.0+editable) (8.1.4)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.15.4->deepsight==0.1.0+editable) (3.1.32)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.15.4->deepsight==0.1.0+editable) (1.28.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.15.4->deepsight==0.1.0+editable) (0.4.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.15.4->deepsight==0.1.0+editable) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.15.4->deepsight==0.1.0+editable) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.15.4->deepsight==0.1.0+editable) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.15.4->deepsight==0.1.0+editable) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.15.4->deepsight==0.1.0+editable) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.15.4->deepsight==0.1.0+editable) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.15.4->deepsight==0.1.0+editable) (4.0.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=4.30.2->deepsight==0.1.0+editable) (2023.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics>=8.0.131->deepsight==0.1.0+editable) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics>=8.0.131->deepsight==0.1.0+editable) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics>=8.0.131->deepsight==0.1.0+editable) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics>=8.0.131->deepsight==0.1.0+editable) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics>=8.0.131->deepsight==0.1.0+editable) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics>=8.0.131->deepsight==0.1.0+editable) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics>=8.0.131->deepsight==0.1.0+editable) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai>=0.27.8->deepsight==0.1.0+editable) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai>=0.27.8->deepsight==0.1.0+editable) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai>=0.27.8->deepsight==0.1.0+editable) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai>=0.27.8->deepsight==0.1.0+editable) (3.4)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations>=1.3.1->deepsight==0.1.0+editable) (2.25.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations>=1.3.1->deepsight==0.1.0+editable) (2023.7.4)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations>=1.3.1->deepsight==0.1.0+editable) (1.4.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing_inspect>=0.4.0->pyserde>=0.11.0->deepsight==0.1.0+editable) (1.0.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai>=0.27.8->deepsight==0.1.0+editable) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai>=0.27.8->deepsight==0.1.0+editable) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai>=0.27.8->deepsight==0.1.0+editable) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai>=0.27.8->deepsight==0.1.0+editable) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai>=0.27.8->deepsight==0.1.0+editable) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai>=0.27.8->deepsight==0.1.0+editable) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyserde>=0.11.0->deepsight==0.1.0+editable) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.1->deepsight==0.1.0+editable) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.15.4->deepsight==0.1.0+editable) (5.0.0)\n",
            "Building wheels for collected packages: deepsight\n",
            "  Building editable for deepsight (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepsight: filename=deepsight-0.1.0+editable-py3-none-any.whl size=2291 sha256=8fccc9d4108454f63d37ba63dc8812fcb21e930d391b1e2d640a50176bba8f24\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-t1h2w8ev/wheels/a4/c1/cb/2b7498ab9f18c5dfab488a932e6013af5a3e7a1f787108d788\n",
            "Successfully built deepsight\n",
            "Installing collected packages: numpy, scikit-learn, albumentations, deepsight\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 1.2.1\n",
            "    Uninstalling albumentations-1.2.1:\n",
            "      Successfully uninstalled albumentations-1.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.1 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed albumentations-1.3.1 deepsight-0.1.0+editable numpy-1.25.1 scikit-learn-1.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHZyO_vTKpGU"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_25QxfxKpGV"
      },
      "source": [
        "Referring Expression Comprehension (REC) is the task of localizing a target object in an image given a natural language expression that refers to it. Most recent approaches ([Zhang et al. 2022](https://arxiv.org/abs/2206.05836), [Xu et al. 2023](https://arxiv.org/abs/2302.00402), [Liu et al. 2023](https://arxiv.org/abs/2303.05499)) that obtain state-of-the-art results on this task are not specifically designed for it, but they are designed to solve a large variety of tasks that require fusing vision and language modalities, like open-set object detection, image captioning, visual question answering and so on. In particular, most of these first independently encode the the visual and textual input using vision and text encoders (based on the Transformer architecture) respectively, then another transformer module is used to fuse the two modalities by making the visual features attend to the textual features and vice versa. Finally, the fused features are given in input to another module (a simple head, a transformer decoder, etc.) based on the task that is being solved.\n",
        "\n",
        "Here we argue that the task of REC requires an high-level understanding of the scene described the region caption. For example, given a caption like *\"The girl approaching the table while holding a glass\"*, to correctly localize the associated bounding box, we need to first identify all the entities referred by the sentence (`the girl`, `the table`, `a glass`) and the relation that exist among them ((`the girl` -- `approaching` -> `the table`), (`the girl` -- `holding` --> `a glass`)). In other words, we need to extract from the sequence of words that form the sentence an intermediate higher-level representation of the scene. Then, instead of grounding the sequence of words to the image, we can ground the intermediate representation to the image. On the other hand, previously cited approaches, since they need to generalize to many image-text tasks, simply ground the word features (here we make the simplifying assumption that each token correspond to a word) extracted by the text encoder to the image features. Thus, to perform well in such task, the text encoder need to encode into each token not only the meaning of the corresponding word but also its relations with the other entities that in the sentence may be refered by group of tokens. In toher words, the text encoder need to learn to extract from the sequence of tokens a higher-level representation without being explicitly supervised to do so.\n",
        "\n",
        "Based on this observation, we propose a new approach to REC that is specifically designed for this task by making the network directly exploit the higher-level semantic information encoded in the input sentence. In particular, from the input sentence we extract a scene graph representing which entities are present in the region and how they are related to each other. Then, we localize the target region by localizing in the image the referred entities that satisfy the referred relations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y75JY1ghKpGW"
      },
      "source": [
        "### High level architecture overview\n",
        "\n",
        "Our architecture is highly inspired to DETR-like models ([Carion et al. 2020](https://arxiv.org/abs/2005.12872), [Gao et al. 2021](https://arxiv.org/abs/2101.07448), [Liu et al. 2022](https://arxiv.org/abs/2201.12329)). Such models use a (CNN) backbone followed by a transformer-based vision encoder to extract visual features from the input image. Then a set of queries (representing candidate bounding boxes) is given in input to a transformer-based decoder, where such queries go through layers of self-attention and cross-attention with the visual features. Finally, the output of the decoder is given in input to a simple head that predicts the bounding boxes coordinates and the class of each bounding box. At training time, Hungarian matching is used to obtain a one-to-one matching between a query and a ground truth bounding box. Once such association is obtained, the loss is computed by comparing the predicted bounding box with the associated ground truth bounding box. At inference time, the predicted bounding boxes are filtered by a simple post-processing step to remove the predicted bounding boxes that have a low confidence score.\n",
        "\n",
        "Similarly, we extract the visual features by employing a transformer-based vision encoder (no backbone is used since we use the CLIP vision encoder). Then, differently from DETR-like models, we do not generate a predefined set of fixed or learnable queries, but we create a graph based on the one extracted from the sentence. In particular, for each entity in the sentence scene graph we create multiple nodes in the graph (since in the image there may be multiple instances of the same entity) whose embeddings are initialized with the embedding obtained by giving in input to the CLIP text encoder the entity textual description extracted from the sentence. Then, we create an edge between two nodes if the corresponding entities are related in the sentence scene graph; as before, the edge features are initialized by encoding the textual description of the relation with the CLIP text encoder. Then, the generated graph is given in input to the transformer-based decoder whose blocks consist of a sequence of `Multi-Head Cross Attention`, `Graph Attention` and `FFN`.\n",
        "\n",
        "In the multi-head cross attention layer, each query (nodes + edges) can attend to the visual features extracted from the vision encoder. This allow each query to verify whether the associated entity/relation is present in a specific region of the image. Then, in the graph attention layer, each node can communicate with its neighbours and the associated relations to verify whether the encoded instance of the entity satisfy the relations encoded in the sentence scene graph.\n",
        "\n",
        "Finally, from the graph outputted by the decoder, we extract the nodes that represent the subject of the sentence (i.e. the target entity) and we give them in input to a simple head to obtain candidate bounding boxes for the target entity. At training time, a simple matching algorithm is applied to associate the ground truth bounding box with one of the predicted bounding boxes and the loss is computed. At inference time, we select the node (and the obtained bounding box) whose embedding is the most similar to the embedding obtained by giving in input to the CLIP text encoder the full sentence.\n",
        "\n",
        "As currently presented, the decoder should also perform open-set object detection, since for each entity it should localize all the instances in the image. Thus, we should create a sufficient high number of nodes for each entity to be able to localize all the instances of the entity in the image. For example, Grounding DINO ([Liu et al. 2023](https://arxiv.org/abs/2303.05499)) creates 900 queries for each image. This would clearly require a lot of memory and computation power. Furthermore, current open-set object detectors are trained on huge amounts of data, on many GPUs and for long period of times (Grounding DINO uses 64 A100). Given the limited resources availables, we decided to employ an open-set object detector to obtain all instances of an entity in the image and an estimate of their location. In this way the decoder does not need to perform open-set object detection from scratch but it only needs to refine the estimated locations. Since existing open-set object detectors are mainly trained on closed object detection datasets, where each entity to be detected is represented by a single noun (i.e., the category name), to make the detector localize an entity we do not use its full textual description. Instead, for each entity we extract a single noun that best describe that entity. For example, given the entity `The woman with dark hair`, we extract the noun `woman` to localize the entity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BiKPN9uKpGY"
      },
      "source": [
        "## Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO_onX67KpGY"
      },
      "source": [
        "### How to extract scene graphs?\n",
        "\n",
        "As previously said, one of the first step is the extraction of the region scene graph from its text description. This task can be seen as the union of two closely related problems: named entity recognition and relation extraction. Since these tasks have long been studied by the NLP community, we have tried many existing solutions or we took inspiration from them to build our own. In the following we will describe the main approaches we have tried.\n",
        "\n",
        "Before diving into details, we notice that the generation of a scene graph from a sentence is an ambiguous task, that is the same sentence could be parsed into different scene graphs. When two noun phrases are connected by an action verb, it seems obvious to identify each noun as an entity and the verb as a relation. However, when the nouns are connected by a preposition, the situation is more ambiguous. For example, in the sentence *\"the woman in a green shirt\"*, the noun phrase `a green shirt` could be considered an attribute of `the woman` or a different entity related to `the woman` with the relation `in`/`wearing`. Similarly, in the sentence *\"the woman on the right\"*, some people may consider `the right` as an actual physical location and thus as an entity, while others may consider it as an attribute of `the woman`.\n",
        "\n",
        "Since most phrases in the RefCOCOg dataset are quite short, if we preferred the attribute interpretation, we would have obtained many scene graphs with very few nodes and edges or no edges at all, thus jeopardizing the idea underlying the model. For this reason, we have generally preferred the creation of a new entity for each noun phrase. However, we have preferred the attribute interpretation, when we thought that the detector would find it difficult to localize such entity (for example, in the case of spatial locations like `the right`, `the left`, `the background`, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdV14CZ7KpGZ"
      },
      "source": [
        "#### Dependency Graph based parsers\n",
        "\n",
        "Historically, one of the first approach to parse sentences in natural language into scene graphs was the one proposed by ([Schuster et al. 2015](https://aclanthology.org/W15-2812.pdf)). First the sentence is parsed into a semantic graph (i.e., a dependency graph to which some refinements are applied, like the handling of pronouns and plural nouns) using the CoreNLP pipeline. Then, based on a set of human-written rules, the semantic graph is converted into a scene graph. This approach has been used in many past works for different purposes, like evaluating generated image captions ([Anderson et al. 2016](https://arxiv.org/abs/1607.08822)) or creating pseudo ground truth scene graphs for Weakly Supervised Scene Graph Generation ([Ye et al. 2021](https://arxiv.org/abs/2105.13994), [Zhong et al. 2021](https://arxiv.org/abs/2109.02227), [Li et al. 2022](https://arxiv.org/abs/2208.01834)).\n",
        "\n",
        "Note that, since the original parser is written in Java, we used a Python-based [tool](https://github.com/vacancy/SceneGraphParser) that covers all the rules implemented in the Stanford Parser with some additional ones (however, it does not implement some features like pronoun handling and quantificational modifiers).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmAsFKKOKpGa",
        "outputId": "75d4ab64-9e9c-453c-c19e-c20fff96d28e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Entity(noun='girl', phrase='The girl'), Entity(noun='table', phrase='the table')]\n",
            "[Triplet(subject=0, relation='approaching', object=1)]\n"
          ]
        }
      ],
      "source": [
        "import sng_parser\n",
        "\n",
        "from deepsight.data.structs import SceneGraph, Entity, Triplet\n",
        "\n",
        "gdict = sng_parser.parse(\"The girl approaching the table\")\n",
        "\n",
        "graph = SceneGraph.new(\n",
        "    entities=[Entity(ent['head'], ent['span']) for ent in gdict[\"entities\"]],\n",
        "    triplets=[Triplet(trip['subject'], trip['relation'], trip['object']) for trip in gdict[\"relations\"]]\n",
        ")\n",
        "\n",
        "print(graph.entities())\n",
        "print(graph.triplets(None, True, False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf5eDmmcKpGc"
      },
      "source": [
        "Despite being largely used by many previous works, we noticed that the quality of the parsed scene graphs rapidly degrades as the strcuture of the sentence becomes more distant from *subject* *predicate* *object*. For example, a sentence like \"the girl approaching the table\" is correctly parsed as we can see from the previous python snippet.\n",
        "\n",
        "However, as the sentence becomes more complex, many entities are not found or prepositions/adjectives are classified as entities. Similarly, many relations are missing or the wrong relation is assigned to a pair of entities. For example, if we simply extend the previous sentence with a coordinate conjunction (\"while holding a glass\"), the parser completely ignores the relation `(0, \"holding\", 2)`, making the entity `the glass` not present in the scene graph (as it will be described later, the generated scene graph will be pruned to remove entities not connected to the subject of the description)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Y9dQFdJKpGd",
        "outputId": "67ab4bc4-6ba7-4f63-f54b-981642492780",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Entity(noun='girl', phrase='The girl'), Entity(noun='table', phrase='the table'), Entity(noun='glass', phrase='a glass')]\n",
            "[Triplet(subject=0, relation='approaching', object=1)]\n"
          ]
        }
      ],
      "source": [
        "gdict = sng_parser.parse(\"The girl approaching the table while holding a glass\")\n",
        "\n",
        "graph = SceneGraph.new(\n",
        "    entities=[Entity(ent['head'], ent['span']) for ent in gdict[\"entities\"]],\n",
        "    triplets=[Triplet(trip['subject'], trip['relation'], trip['object']) for trip in gdict[\"relations\"]]\n",
        ")\n",
        "\n",
        "print(graph.entities())\n",
        "print(graph.triplets(None, True, False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GLd4kGVKpGe"
      },
      "source": [
        "Similarly, if we consider the sentence *\"There is a truck covered in snow farthest from the right\"*, the parser completely ignores the entity `the snow` and the relation `(0, \"covered in\", 2)`. Furthermore, the parser wrongly classified the expression `farthest from the right` as relation + entity, when they should be considered attributes of the entity `the truck`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60tWTo1cKpGg",
        "outputId": "99a64bf7-25b0-427b-e5d3-5074e99559ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Entity(noun='truck', phrase='a truck'), Entity(noun='right', phrase='the right')]\n",
            "[Triplet(subject=0, relation='from', object=1)]\n"
          ]
        }
      ],
      "source": [
        "gdict = sng_parser.parse(\"There is a truck covered in snow farthest from the right\")\n",
        "\n",
        "graph = SceneGraph.new(\n",
        "    entities=[Entity(ent['head'], ent['span']) for ent in gdict[\"entities\"]],\n",
        "    triplets=[Triplet(trip['subject'], trip['relation'], trip['object']) for trip in gdict[\"relations\"]]\n",
        ")\n",
        "\n",
        "print(graph.entities())\n",
        "print(graph.triplets(None, True, False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-_fG4VBKpGg"
      },
      "source": [
        "By analyzing the functioning of the parser, we noticed that its poor results are mainly due to the NLP pipeline used for the generation of the dependency graph and to the limited set of rules used to convert the dependency graph into a scene graph. In particular, due to the low quality of many sentences in the dataset (e.g., typos, not perfect syntactic structure), the dependency graph generated by the spaCy `en_core_web_sm` pipeline in many cases assign the wrong universal dependency relation tag between two words, thus leading to the wrong conversion into a scene graph.\n",
        "\n",
        "Thus, since the quality of the scene graph is paramount for the success of the model, we tried to develop a new tool using a more powerful dependency parser and a more refined set of rules. In particular, we used one of the state-of-the-art dependency parsers by ([Attardi et al. 2022](https://github.com/Unipisa/diaparser)), that extends the architecture of the Biaffine Parser by exploiting both embeddings and attentions provided by transformers. Then, based on the dependency graphs generated for some sentences of the dataset and the corresponding ground-truth scene graphs, we developed a new set of rules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mITT9LRHKpGg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319,
          "referenced_widgets": [
            "86f4401400f24e558c12c0838dacf0a5",
            "ce0991cd9f924c18a0ccd2ee3dffc918",
            "c84b4bb7566f4ebca2d02a24ce68900f",
            "c1903c33b24a46df8abdc84ece029b46",
            "b12a6bd898314bcaa8509a114a6a2fda",
            "96d910be16694d37b7f2d9f694ce9482",
            "67336c6b893342f9afad1cb8d43fc744",
            "937a571c99384c4098f8ba42dd8f0ac7",
            "9ebb2dec18d44624aa98ebc4d0d3d9c8",
            "21ab4c465f814b32af577748989bfe7a",
            "e7ab3e218be642689e3eddde0a4b6d40",
            "90bdbb4ac7f64987b474625440df8385",
            "e4ed0dd2961540c190adb2c78f65b1a3",
            "0f5f82f31f714bc0b2da4382e38414dc",
            "46f970d47fa549189ae3d4e528f2e498",
            "f453ef1662b247ea84725a6c610d2d8a",
            "3b9332538dff406cac3f0a4603bfd0b5",
            "c7815161ca3f4551a9db00e3852956e2",
            "7e0f0ebb09a84b30a63f06483e0a9033",
            "288cc12fd6404bdeb6edf98123d0ec05",
            "895f1778658640e5859d8b557ab1becc",
            "a9966204462b4c82ad99933db6e53714",
            "78566cfde03f479985ccdb89d41b832b",
            "1c22003fcf7b4e3aad677ba281619a7c",
            "9d2214bad7054a288d8b0badf54cf1a7",
            "ca9337525a1b4b04a369717d3af4c4e5",
            "a36c5cfea94e4f1c9fceceee902d341c",
            "8a8d3ba5929d47439a59796ab4fc86d0",
            "eeaaa228597b40e988d31afa8f99d365",
            "69ff9de76ea34cedbbc09d8f28838cfe",
            "adc5d768b80b4d38905505c0d12e3ef1",
            "f0f9c385be0a44c3937bdaf52bb16ad8",
            "9ba897e6ef3f494686b4461eac2b25eb",
            "e1ea8b9202fc47458987a1040396a0b5",
            "6e02ba7ab66143d2b143e2e8ece32456",
            "7b26b558b2c248c096aca6c655a6c31d",
            "4911ca1568f442cb95583d8c274486c2",
            "debc059cf70740d985d33399f725ec9b",
            "3efe0417484341a38fecdbd657222cc3",
            "dee37c7187c34689bf9df559f2f6a40f",
            "7772094ba3d6405581042233a20b4d28",
            "7bbb9cc9dc694d8bad724f49074f4037",
            "bac244b54e9c4dbdb70cdace8f049eeb",
            "152a1b173ca1424abebb5484a3327fee",
            "1963cfda021540a0bf407f49671da25f",
            "f94a3ccef30b4b93b7fc4825713cb1d1",
            "b1fbede332db4b6e90def84ff70903a1",
            "27414f0c0c884b6abf1c4ea4f5e11d1b",
            "728a5a956ae448d0aef8efad986689b2",
            "36804f47d197479ea892d282faaa28cf",
            "27fbf01336474e1c9c8a53faea1ffdf2",
            "1b5d8077a07646d2ba8d5f3a34a51e78",
            "e13c07b568e841b7b99accf83a2b77c9",
            "f86d2a198e3e4907a08fab52aa1514df",
            "82f1567473a049779bfa0dcea54481b8"
          ]
        },
        "outputId": "5f6eeadf-bb01-4903-d37e-3acd91f51fee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/Unipisa/diaparser/releases/download/v1.0/en_ewt.electra-base\" to /root/.cache/diaparser/en_ewt.electra-base\n",
            "100%|██████████| 452M/452M [00:28<00:00, 16.7MB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86f4401400f24e558c12c0838dacf0a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90bdbb4ac7f64987b474625440df8385"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
            "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/27.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78566cfde03f479985ccdb89d41b832b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1ea8b9202fc47458987a1040396a0b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1963cfda021540a0bf407f49671da25f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using bos_token, but it is not set yet.\n",
            "Using eos_token, but it is not set yet.\n"
          ]
        }
      ],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "from diaparser.parsers import Parser\n",
        "import rustworkx as rx\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Word:\n",
        "    tag: str\n",
        "    text: str\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class DepEntity:\n",
        "    head: list[int]\n",
        "    others: list[int]\n",
        "\n",
        "\n",
        "class ERParser:\n",
        "    def __init__(self) -> None:\n",
        "        self._parser = Parser.load(\"en_ewt-electra\")\n",
        "\n",
        "    def _get_dependency_graph(self, sentence: str) -> rx.PyDiGraph:\n",
        "        dataset = self._parser.predict(sentence, text=\"en\")\n",
        "        tokens = dataset.sentences[0].to_tokens()\n",
        "\n",
        "        graph = rx.PyDiGraph()  # type: ignore\n",
        "\n",
        "        graph.add_node(Word(\"ROOT\", \"-ROOT-\"))\n",
        "        graph.add_nodes_from([Word(token[\"deprel\"], token[\"form\"]) for token in tokens])\n",
        "\n",
        "        for token in tokens:\n",
        "            head_id = int(token[\"head\"])\n",
        "            id = int(token[\"id\"])\n",
        "            graph.add_edge(head_id, id, None)\n",
        "\n",
        "        return graph\n",
        "\n",
        "    def _get_child_by_tag(self, graph: rx.PyDiGraph, node: int, tag: str) -> list[int]:\n",
        "        children = []\n",
        "\n",
        "        for child_id in graph.neighbors(node):\n",
        "            if tag in graph.get_node_data(child_id).tag:\n",
        "                children.append(child_id)\n",
        "\n",
        "        return children\n",
        "\n",
        "    def _compose_span(self, dep_graph: rx.PyDiGraph, words_ids: list[int]) -> str:\n",
        "        words_ids.sort()\n",
        "        words = [dep_graph.get_node_data(id).text for id in words_ids]\n",
        "        return \" \".join(words)\n",
        "\n",
        "    def _get_all_children(self, dep_graph: rx.PyDiGraph, node_id: int) -> list[int]:\n",
        "        children = []\n",
        "        for child_id in dep_graph.neighbors(node_id):\n",
        "            children.append(child_id)\n",
        "            children.extend(self._get_all_children(dep_graph, child_id))\n",
        "        return children\n",
        "\n",
        "    def _compose(\n",
        "        self,\n",
        "        orig: rx.PyDiGraph,\n",
        "        other: rx.PyDiGraph,\n",
        "        parent_id: int,\n",
        "        rel_ids: list[int],\n",
        "    ) -> None:\n",
        "        roots = []\n",
        "        for node_id in other.node_indexes():\n",
        "            if other.in_degree(node_id) == 0:\n",
        "                roots.append(node_id)\n",
        "\n",
        "        if len(roots) == 0:\n",
        "            raise ValueError(\"No root found\")\n",
        "\n",
        "        new_node_ids = orig.compose(other, {})\n",
        "\n",
        "        for root_id in roots:\n",
        "            orig.add_edge(parent_id, new_node_ids[root_id], rel_ids)\n",
        "\n",
        "    def _get_coordinated_verbs(self, dep_graph: rx.PyDiGraph, verb_id: int) -> list[int]:\n",
        "        verbs = [verb_id]\n",
        "        for child_id in dep_graph.neighbors(verb_id):\n",
        "            tag = dep_graph.get_node_data(child_id).tag\n",
        "            if \"conj\" in tag or \"parataxis\" in tag:\n",
        "                cc_ids = self._get_child_by_tag(dep_graph, child_id, \"cc\")\n",
        "                for cc_id in cc_ids:\n",
        "                    dep_graph.remove_edge(child_id, cc_id)\n",
        "\n",
        "                punct_ids = self._get_child_by_tag(dep_graph, child_id, \"punct\")\n",
        "                for punct_id in punct_ids:\n",
        "                    dep_graph.remove_edge(child_id, punct_id)\n",
        "\n",
        "                dep_graph.remove_edge(verb_id, child_id)\n",
        "                verbs.append(child_id)\n",
        "        return verbs\n",
        "\n",
        "    def _parse_noun(self, dep_graph: rx.PyDiGraph, noun_id: int) -> rx.PyDiGraph:\n",
        "        graph = rx.PyDiGraph()\n",
        "\n",
        "        noun_ids = DepEntity([noun_id], [])\n",
        "        noun_node_id = graph.add_node(noun_ids)\n",
        "\n",
        "        for child_id in dep_graph.neighbors(noun_id):\n",
        "            child = dep_graph.get_node_data(child_id)\n",
        "\n",
        "            if dep_graph.out_degree(child_id) == 0:\n",
        "                if \"compound\" in child.tag:\n",
        "                    noun_ids.head.append(child_id)\n",
        "                else:\n",
        "                    noun_ids.others.append(child_id)\n",
        "                continue\n",
        "\n",
        "            if \"det\" in child.tag:\n",
        "                # this determiner should have no children\n",
        "                raise NotImplementedError\n",
        "            elif \"amod\" in child.tag:\n",
        "                obl_ids = self._get_child_by_tag(dep_graph, child_id, \"obl\")\n",
        "                if len(obl_ids) == 0:\n",
        "                    noun_ids.others.append(child_id)\n",
        "                    noun_ids.others.extend(self._get_all_children(dep_graph, child_id))\n",
        "                else:\n",
        "                    for obl_id in obl_ids:\n",
        "                        dep_graph.remove_edge(child_id, obl_id)\n",
        "                        case_ids = self._get_child_by_tag(dep_graph, obl_id, \"case\")\n",
        "                        for case_id in case_ids:\n",
        "                            dep_graph.remove_edge(obl_id, case_id)\n",
        "\n",
        "                    rel_ids = [child_id, *case_ids]\n",
        "                    for obl_id in obl_ids:\n",
        "                        sub_graph = self._parse_noun(dep_graph, obl_id)\n",
        "                        self._compose(graph, sub_graph, noun_node_id, rel_ids)\n",
        "            elif \"compound\" in child.tag:\n",
        "                raise NotImplementedError\n",
        "            elif \"nmod\" in child.tag or \"obl\" in child.tag:\n",
        "                case_ids = self._get_child_by_tag(dep_graph, child_id, \"case\")\n",
        "                if len(case_ids) == 0:\n",
        "                    raise ValueError(\"No case found\")\n",
        "                for case_id in case_ids:\n",
        "                    dep_graph.remove_edge(child_id, case_id)\n",
        "                    sub_graph = self._parse_noun(dep_graph, child_id)\n",
        "                    self._compose(graph, sub_graph, noun_node_id, [case_id])\n",
        "            elif \"acl:relcl\" in child.tag:\n",
        "                verb_ids = self._get_coordinated_verbs(dep_graph, child_id)\n",
        "                for verb_id in verb_ids:\n",
        "                    nsubj_ids = self._get_child_by_tag(dep_graph, verb_id, \"nsubj\")\n",
        "                    if len(nsubj_ids) == 0:\n",
        "                        raise ValueError(\"No nsubj found\")\n",
        "                    if len(nsubj_ids) > 1:\n",
        "                        raise ValueError(\"More than one nsubj found\")\n",
        "\n",
        "                    dep_graph.remove_edge(verb_id, nsubj_ids[0])\n",
        "                    rel_ids, sub_graph = self._parse_verb(dep_graph, verb_id)\n",
        "                    if sub_graph.num_nodes() == 0:\n",
        "                        noun_ids.others.extend(rel_ids)\n",
        "                    else:\n",
        "                        self._compose(graph, sub_graph, noun_node_id, rel_ids)\n",
        "            elif \"acl\" in child.tag or \"root\" in child.tag:\n",
        "                verb_ids = self._get_coordinated_verbs(dep_graph, child_id)\n",
        "                for verb_id in verb_ids:\n",
        "                    rel_ids, sub_graph = self._parse_verb(dep_graph, verb_id)\n",
        "                    if sub_graph.num_nodes() == 0:\n",
        "                        noun_ids.others.extend(rel_ids)\n",
        "                    else:\n",
        "                        self._compose(graph, sub_graph, noun_node_id, rel_ids)\n",
        "            elif \"conj\" in child.tag:\n",
        "                cc_ids = self._get_child_by_tag(dep_graph, child_id, \"cc\")\n",
        "                for cc_id in cc_ids:\n",
        "                    dep_graph.remove_edge(child_id, cc_id)\n",
        "\n",
        "                punct_ids = self._get_child_by_tag(dep_graph, child_id, \"punct\")\n",
        "                for punct_id in punct_ids:\n",
        "                    dep_graph.remove_edge(child_id, punct_id)\n",
        "\n",
        "                sub_graph = self._parse_noun(dep_graph, child_id)\n",
        "                graph.compose(sub_graph, {})\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown tag: {child.tag}\")\n",
        "\n",
        "        return graph\n",
        "\n",
        "    def _parse_verb(\n",
        "        self, dep_graph: rx.PyDiGraph, verb_id: int\n",
        "    ) -> tuple[list[int], rx.PyDiGraph]:\n",
        "        graph = rx.PyDiGraph()\n",
        "        rel_ids = [verb_id]\n",
        "\n",
        "        for child_id in dep_graph.neighbors(verb_id):\n",
        "            tag = dep_graph.get_node_data(child_id).tag\n",
        "\n",
        "            if dep_graph.out_degree(child_id) == 0:\n",
        "                if \"punct\" not in tag:\n",
        "                    rel_ids.append(child_id)\n",
        "                continue\n",
        "\n",
        "            if \"aux\" in tag:\n",
        "                # this auxiliary verb should have no children\n",
        "                raise NotImplementedError\n",
        "            elif \"nsubj\" in tag:\n",
        "                raise ValueError(\"nsubj should be the root\")\n",
        "            elif \"nmod\" in tag or \"obl\" in tag:\n",
        "                case_ids = self._get_child_by_tag(dep_graph, child_id, \"case\")\n",
        "                if len(case_ids) == 0:\n",
        "                    raise ValueError(\"No case found\")\n",
        "\n",
        "                for case_id in case_ids:\n",
        "                    rel_ids.append(case_id)\n",
        "                    dep_graph.remove_edge(child_id, case_id)\n",
        "\n",
        "                sub_graph = self._parse_noun(dep_graph, child_id)\n",
        "                graph.compose(sub_graph, {})\n",
        "            elif \"obj\" in tag:\n",
        "                sub_graph = self._parse_noun(dep_graph, child_id)\n",
        "                graph.compose(sub_graph, {})\n",
        "            elif \"conj\" in tag:\n",
        "                raise ValueError(\"conj should have been removed\")\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown tag {tag}\")\n",
        "\n",
        "        return rel_ids, graph\n",
        "\n",
        "    def parse(self, sentence: str) -> SceneGraph:\n",
        "        dep_graph = self._get_dependency_graph(sentence)\n",
        "\n",
        "        root = list(dep_graph.out_edges(0))[0][1]\n",
        "        dep_graph.remove_node(0)\n",
        "\n",
        "        nsubj_ids = self._get_child_by_tag(dep_graph, root, \"nsubj\")\n",
        "        if len(nsubj_ids) == 0:\n",
        "            tmp = self._parse_noun(dep_graph, root)\n",
        "        elif len(nsubj_ids) > 1:\n",
        "            raise ValueError(\"More than one nsubj found\")\n",
        "        else:\n",
        "            dep_graph.remove_edge(root, nsubj_ids[0])\n",
        "            dep_graph.add_edge(nsubj_ids[0], root, None)\n",
        "            tmp = self._parse_noun(dep_graph, nsubj_ids[0])\n",
        "\n",
        "        entities = []\n",
        "        for span_ids in tmp.nodes():\n",
        "            head = self._compose_span(dep_graph, span_ids.head)\n",
        "            span = self._compose_span(dep_graph, span_ids.head + span_ids.others)\n",
        "\n",
        "            entities.append(Entity(head, span))\n",
        "\n",
        "        relations = []\n",
        "        for edge in tmp.edge_list():\n",
        "            subject = edge[0]\n",
        "            object = edge[1]\n",
        "            span_ids = tmp.get_edge_data(subject, object)\n",
        "            predicate = self._compose_span(dep_graph, span_ids)\n",
        "\n",
        "            relations.append(Triplet(subject, predicate, object))\n",
        "\n",
        "        return SceneGraph.new(entities, relations)\n",
        "\n",
        "\n",
        "parser = ERParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlYeLMMvKpGh",
        "outputId": "2355dcab-a705-42c2-b568-4c56a3703576",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Entity(noun='boy', phrase='The boy'), Entity(noun='shirt', phrase='a white shirt'), Entity(noun='friends', phrase='his friends')]\n",
            "[Triplet(subject=0, relation='wearing', object=1), Triplet(subject=0, relation='having dinner with', object=2)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:28: UserWarning: apply_permutation is deprecated, please use tensor.index_select(dim, permutation) instead\n",
            "  warnings.warn(\"apply_permutation is deprecated, please use tensor.index_select(dim, permutation) instead\")\n"
          ]
        }
      ],
      "source": [
        "graph = parser.parse(\"The boy wearing a white shirt having dinner with his friends\")\n",
        "\n",
        "print(graph.entities())\n",
        "print(graph.triplets(None, True, False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLZj9x3iKpGi"
      },
      "source": [
        "Despite the promising results obtained on some sentences, we abandoned this approach for two reasons:\n",
        "1. The quality of the resulting scene graph depends too much on the quality of the dependency graph generated by the parser. In particular, even though the new parser correclty parses more complex sentences, it still fails at handling long-range connections between words or relations that can be inferred from common sense. For example, in the sentence *\"There is a truck covered in snow farthest from the right\"*, the parser connects the clause `farthest from the right` to the clause `covered in snow`. While this interpretation of the sentence may be deemed right (the sentence is ambiguous), the preferred interpretation should be that `farthest from the right` is an attribute of the entity `the truck`. However, the parser is not able to infer this relation, thus leading to the wrong scene graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1YZ0a89KpGi",
        "outputId": "59456d06-2d9f-449e-8f61-d870b9e1c221",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"3d0ad7eb04414b20a270771ec3094cab-0\" class=\"displacy\" width=\"1490\" height=\"317.0\" direction=\"ltr\" style=\"max-width: none; height: 317.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">ROOT</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">There</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">truck</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">covered</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">in</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"890\">snow</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"890\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1010\">farthest</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1010\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1130\">from</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1130\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1250\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1250\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1370\">right</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1370\"></tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3d0ad7eb04414b20a270771ec3094cab-0-0\" stroke-width=\"2px\" d=\"M182,182.0 182,162.0 284.0,162.0 284.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3d0ad7eb04414b20a270771ec3094cab-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">expl</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M182,184.0 L178,176.0 186,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3d0ad7eb04414b20a270771ec3094cab-0-1\" stroke-width=\"2px\" d=\"M62,182.0 62,142.0 287.0,142.0 287.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3d0ad7eb04414b20a270771ec3094cab-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">root</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M287.0,184.0 L291.0,176.0 283.0,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3d0ad7eb04414b20a270771ec3094cab-0-2\" stroke-width=\"2px\" d=\"M422,182.0 422,162.0 524.0,162.0 524.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3d0ad7eb04414b20a270771ec3094cab-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M422,184.0 L418,176.0 426,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3d0ad7eb04414b20a270771ec3094cab-0-3\" stroke-width=\"2px\" d=\"M302,182.0 302,142.0 527.0,142.0 527.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3d0ad7eb04414b20a270771ec3094cab-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M527.0,184.0 L531.0,176.0 523.0,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3d0ad7eb04414b20a270771ec3094cab-0-4\" stroke-width=\"2px\" d=\"M542,182.0 542,162.0 644.0,162.0 644.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3d0ad7eb04414b20a270771ec3094cab-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acl</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M644.0,184.0 L648.0,176.0 640.0,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3d0ad7eb04414b20a270771ec3094cab-0-5\" stroke-width=\"2px\" d=\"M782,182.0 782,162.0 884.0,162.0 884.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3d0ad7eb04414b20a270771ec3094cab-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M782,184.0 L778,176.0 786,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3d0ad7eb04414b20a270771ec3094cab-0-6\" stroke-width=\"2px\" d=\"M662,182.0 662,142.0 887.0,142.0 887.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3d0ad7eb04414b20a270771ec3094cab-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M887.0,184.0 L891.0,176.0 883.0,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3d0ad7eb04414b20a270771ec3094cab-0-7\" stroke-width=\"2px\" d=\"M662,182.0 662,122.0 1010.0,122.0 1010.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3d0ad7eb04414b20a270771ec3094cab-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1010.0,184.0 L1014.0,176.0 1006.0,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3d0ad7eb04414b20a270771ec3094cab-0-8\" stroke-width=\"2px\" d=\"M1142,182.0 1142,142.0 1367.0,142.0 1367.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3d0ad7eb04414b20a270771ec3094cab-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1142,184.0 L1138,176.0 1146,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3d0ad7eb04414b20a270771ec3094cab-0-9\" stroke-width=\"2px\" d=\"M1262,182.0 1262,162.0 1364.0,162.0 1364.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3d0ad7eb04414b20a270771ec3094cab-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1262,184.0 L1258,176.0 1266,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3d0ad7eb04414b20a270771ec3094cab-0-10\" stroke-width=\"2px\" d=\"M1022,182.0 1022,122.0 1370.0,122.0 1370.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3d0ad7eb04414b20a270771ec3094cab-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1370.0,184.0 L1374.0,176.0 1366.0,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from spacy import displacy\n",
        "\n",
        "sent = parser._parser.predict(\"There is a truck covered in snow farthest from the right\", text=\"en\").sentences[0]\n",
        "displacy.render(sent.to_displacy(), style='dep', manual=True, options={'compact': True, 'distance': 120}, jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HewO3vJKpGj"
      },
      "source": [
        "2. Defining a set of universal rules to handle all possible structures of English sentences (even malformed ones) is extremely hard. In particular, there are many sentences that have very similar dependency graphs that, however, should be transformed in different scene graphs, thus requiring not only to handle the structure of the sentence, but also its semantics. For example, the expressions *\"the part of the table\"*, *\"the first horse from the left\"* and *\"the woman in green clothes\"*, have very similar dependency graphs, but in the first two cases the expressions should be considered a single entity, while in the last case they should be considered two different entities related by the relation `wearing`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpIF-MKiKpGj",
        "outputId": "b6fca859-9e91-4100-d4c6-e3d62c1a021c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"8a82f162f7384a3c95af7c3d53f0257c-0\" class=\"displacy\" width=\"770\" height=\"317.0\" direction=\"ltr\" style=\"max-width: none; height: 317.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">ROOT</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">part</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">of</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">table</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\"></tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-8a82f162f7384a3c95af7c3d53f0257c-0-0\" stroke-width=\"2px\" d=\"M182,182.0 182,162.0 284.0,162.0 284.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-8a82f162f7384a3c95af7c3d53f0257c-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M182,184.0 L178,176.0 186,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-8a82f162f7384a3c95af7c3d53f0257c-0-1\" stroke-width=\"2px\" d=\"M62,182.0 62,142.0 287.0,142.0 287.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-8a82f162f7384a3c95af7c3d53f0257c-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">root</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M287.0,184.0 L291.0,176.0 283.0,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-8a82f162f7384a3c95af7c3d53f0257c-0-2\" stroke-width=\"2px\" d=\"M422,182.0 422,142.0 647.0,142.0 647.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-8a82f162f7384a3c95af7c3d53f0257c-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M422,184.0 L418,176.0 426,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-8a82f162f7384a3c95af7c3d53f0257c-0-3\" stroke-width=\"2px\" d=\"M542,182.0 542,162.0 644.0,162.0 644.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-8a82f162f7384a3c95af7c3d53f0257c-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M542,184.0 L538,176.0 546,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-8a82f162f7384a3c95af7c3d53f0257c-0-4\" stroke-width=\"2px\" d=\"M302,182.0 302,122.0 650.0,122.0 650.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-8a82f162f7384a3c95af7c3d53f0257c-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M650.0,184.0 L654.0,176.0 646.0,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "sent = parser._parser.predict(\"the part of the table\", text=\"en\").sentences[0]\n",
        "displacy.render(sent.to_displacy(), style='dep', manual=True, jupyter=True, options={'compact': True, 'distance': 120})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZqzURNXKpGk",
        "outputId": "eb93fbce-2b74-467c-f99a-9bd637662aec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"ed744f0e44284d268865ceaec93fa3ed-0\" class=\"displacy\" width=\"890\" height=\"317.0\" direction=\"ltr\" style=\"max-width: none; height: 317.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">ROOT</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">first</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">horse</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">from</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">left</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\"></tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-ed744f0e44284d268865ceaec93fa3ed-0-0\" stroke-width=\"2px\" d=\"M182,182.0 182,142.0 407.0,142.0 407.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-ed744f0e44284d268865ceaec93fa3ed-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M182,184.0 L178,176.0 186,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-ed744f0e44284d268865ceaec93fa3ed-0-1\" stroke-width=\"2px\" d=\"M302,182.0 302,162.0 404.0,162.0 404.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-ed744f0e44284d268865ceaec93fa3ed-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M302,184.0 L298,176.0 306,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-ed744f0e44284d268865ceaec93fa3ed-0-2\" stroke-width=\"2px\" d=\"M62,182.0 62,122.0 410.0,122.0 410.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-ed744f0e44284d268865ceaec93fa3ed-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">root</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M410.0,184.0 L414.0,176.0 406.0,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-ed744f0e44284d268865ceaec93fa3ed-0-3\" stroke-width=\"2px\" d=\"M542,182.0 542,142.0 767.0,142.0 767.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-ed744f0e44284d268865ceaec93fa3ed-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M542,184.0 L538,176.0 546,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-ed744f0e44284d268865ceaec93fa3ed-0-4\" stroke-width=\"2px\" d=\"M662,182.0 662,162.0 764.0,162.0 764.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-ed744f0e44284d268865ceaec93fa3ed-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M662,184.0 L658,176.0 666,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-ed744f0e44284d268865ceaec93fa3ed-0-5\" stroke-width=\"2px\" d=\"M422,182.0 422,122.0 770.0,122.0 770.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-ed744f0e44284d268865ceaec93fa3ed-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M770.0,184.0 L774.0,176.0 766.0,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "sent = parser._parser.predict(\"the first horse from the left\", text=\"en\").sentences[0]\n",
        "displacy.render(sent.to_displacy(), style='dep', manual=True, jupyter=True, options={'compact': True, 'distance': 120})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTHbwYnsKpGk",
        "outputId": "1868ea28-7cfb-417c-cbe3-59e2afe5a388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"ba674137ee0e4ba5b7ff920a3cba9189-0\" class=\"displacy\" width=\"770\" height=\"317.0\" direction=\"ltr\" style=\"max-width: none; height: 317.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">ROOT</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">woman</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">in</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">green</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\"></tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">clothes</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\"></tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-ba674137ee0e4ba5b7ff920a3cba9189-0-0\" stroke-width=\"2px\" d=\"M182,182.0 182,162.0 284.0,162.0 284.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-ba674137ee0e4ba5b7ff920a3cba9189-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M182,184.0 L178,176.0 186,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-ba674137ee0e4ba5b7ff920a3cba9189-0-1\" stroke-width=\"2px\" d=\"M62,182.0 62,142.0 287.0,142.0 287.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-ba674137ee0e4ba5b7ff920a3cba9189-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">root</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M287.0,184.0 L291.0,176.0 283.0,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-ba674137ee0e4ba5b7ff920a3cba9189-0-2\" stroke-width=\"2px\" d=\"M422,182.0 422,142.0 647.0,142.0 647.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-ba674137ee0e4ba5b7ff920a3cba9189-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M422,184.0 L418,176.0 426,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-ba674137ee0e4ba5b7ff920a3cba9189-0-3\" stroke-width=\"2px\" d=\"M542,182.0 542,162.0 644.0,162.0 644.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-ba674137ee0e4ba5b7ff920a3cba9189-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M542,184.0 L538,176.0 546,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-ba674137ee0e4ba5b7ff920a3cba9189-0-4\" stroke-width=\"2px\" d=\"M302,182.0 302,122.0 650.0,122.0 650.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-ba674137ee0e4ba5b7ff920a3cba9189-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M650.0,184.0 L654.0,176.0 646.0,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "sent = parser._parser.predict(\"the woman in green clothes\", text=\"en\").sentences[0]\n",
        "displacy.render(sent.to_displacy(), style='dep', manual=True, options={'compact': True, 'distance': 120}, jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mad6PubKpGl"
      },
      "source": [
        "#### Large Language Models\n",
        "\n",
        "Large Language Models pretrained on large text corpora have recently shown to be able to reach state-of-the-art results on many NLP tasks for which they were not explicitly trained, like question answering, summarization and text generation. Furthermore, since these models seem to show emergent reasoning capabilities ([Liu et al. 2023](https://arxiv.org/abs/2304.03439)), we considered their use for this task that, as previously said, requires common sense and the ability to understand the scene context described by the sentence.\n",
        "\n",
        "In our first attempts, we tried to use open LLM available on the HuggingFace site, like [Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct), [Alpaca-13B](https://crfm.stanford.edu/2023/03/13/alpaca.html), Stable-Vicuna-13B, given the possibility to use their quantized version and thus running them on single gpus with only 8/16 GB of memory. For all our experiments we adopted a few-shot approach, i.e., in the prompt we described the task and then we provided a set of examples of the task. When generating the prompt, the main tradeoff was between the number of examples (and thus the inference time) and the quality of the generated scene graphs. At the end, we decided to use 5 examples that allowed us to show the model a variety of sentences with different structures and relations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9J6w54UKpGl"
      },
      "outputs": [],
      "source": [
        "prompt = \\\n",
        "\"\"\"\"\\\n",
        "The following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, and a human user, called User. Falcon is extremely good at visualizing and understanding a scene from a short description of it, and can answer questions about the scene. User is a human who is curious about the world, and wants to know more about the entities present in the scene, even if not explicitly stated in the description, and which relations occur among them.\n",
        "In the following interactions, User will make requests in natural language, while Falcon will answer to each of these requests with a well formed JSON. The conversation begins.\n",
        "User: Given the following sentence: \"{sentence1}\", what are the entities present in the scene? What are their relations? For each entity, please provide also a single word that best summarizes it and make sure that the subject of the sentence is the first entity.\n",
        "Falcon: {example1}\n",
        "User: Do the same for the following sentence: \"{sentence2}\".\n",
        "Falcon: {example2}\n",
        "User: Try with this one: \"{sentence3}\".\n",
        "Falcon: {example3}\n",
        "User: Please, do the same also for this sentence: \"{sentence4}\".\n",
        "Falcon: {example4}\n",
        "User: Let's try with this one: \"{sentence5}\".\n",
        "Falcon: {example5}\n",
        "User: Finally, try with this one: \"{sentence6}\".\n",
        "Falcon:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdPuxQYXKpGm"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from typing import Iterable\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "\n",
        "class LLMParser:\n",
        "    def __init__(self) -> None:\n",
        "        name = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "        self._prompt = prompt\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            name,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "        )\n",
        "\n",
        "        self._pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            trust_remote_code=True,\n",
        "            device_map=\"auto\",\n",
        "            batch_size=1, # we found that increasing the batch slows down the generation\n",
        "        )\n",
        "\n",
        "        self._pipe.tokenizer.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "    def _create_prompt(self, sentence: str) -> str:\n",
        "        sentence1 = \"the girl looking at the table full of drinks\"\n",
        "        example1 = {\n",
        "            \"entities\": [\n",
        "                (\"the girl\", \"girl\"),\n",
        "                (\"the table\", \"table\"),\n",
        "                (\"drinks\", \"drinks\"),\n",
        "            ],\n",
        "            \"relations\": [\n",
        "                (0, \"looking at\", 1),\n",
        "                (1, \"full of\", 2),\n",
        "            ],\n",
        "        }\n",
        "\n",
        "        sentence2 = (\n",
        "            \"the man wearing a long sleeved white shirt and a pair of blue jeans \"\n",
        "            \"catching a freesbie\"\n",
        "        )\n",
        "        example2 = {\n",
        "            \"entities\": [\n",
        "                (\"the man\", \"man\"),\n",
        "                (\"a long sleeved white shirt\", \"shirt\"),\n",
        "                (\"a pair of blue jeans\", \"jeans\"),\n",
        "                (\"a freesbie\", \"freesbie\"),\n",
        "            ],\n",
        "            \"relations\": [\n",
        "                (0, \"wearing\", 1),\n",
        "                (0, \"wearing\", 2),\n",
        "                (0, \"catching\", 3),\n",
        "            ],\n",
        "        }\n",
        "\n",
        "        sentence3 = \"Skateboarder in green\"\n",
        "        example3 = {\n",
        "            \"entities\": [\n",
        "                (\"Skateboarder\", \"Skateboarder\"),\n",
        "                (\"green clothes\", \"clothes\"),\n",
        "            ],\n",
        "            \"relations\": [\n",
        "                (0, \"in\", 1),\n",
        "            ],\n",
        "        }\n",
        "\n",
        "        sentence4 = \"glass far right\"\n",
        "        example4 = {\n",
        "            \"entities\": [(\"glass far right\", \"glass\")],\n",
        "            \"relations\": [],\n",
        "        }\n",
        "\n",
        "        sentence5 = \"2nd to theleft brown horse drinking\"\n",
        "        example5 = {\n",
        "            \"entities\": [\n",
        "                (\"brown horse drinking\", \"horse\"),\n",
        "                (\"leftmost brown horse\", \"horse\"),\n",
        "            ],\n",
        "            \"relations\": [\n",
        "                (0, \"at the right of\", 1),\n",
        "            ],\n",
        "        }\n",
        "\n",
        "        return self._prompt.format(\n",
        "            sentence1=sentence1,\n",
        "            example1=json.dumps(example1),\n",
        "            sentence2=sentence2,\n",
        "            example2=json.dumps(example2),\n",
        "            sentence3=sentence3,\n",
        "            example3=json.dumps(example3),\n",
        "            sentence4=sentence4,\n",
        "            example4=json.dumps(example4),\n",
        "            sentence5=sentence5,\n",
        "            example5=json.dumps(example5),\n",
        "            sentence6=sentence,\n",
        "        )\n",
        "\n",
        "    def parse(self, sentences: Iterable[str]) -> Iterable[SceneGraph]:\n",
        "        \"\"\"\"Extracts scene graphs from a list of sentences.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        sentences : Iterable[str]\n",
        "            A list of sentences to parse.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Iterable[SceneGraph]\n",
        "            A list of scene graphs, one for each sentence.\n",
        "        \"\"\"\n",
        "\n",
        "        prompts = (self._create_prompt(s) for s in sentences)\n",
        "\n",
        "        generator = self._pipe(\n",
        "            prompts,\n",
        "            max_length=1000,\n",
        "            num_return_sequences=1,\n",
        "            do_sample=False,\n",
        "            # top_k=10,\n",
        "            return_full_text=False,\n",
        "        )\n",
        "\n",
        "        for output in generator:\n",
        "            generated = output[0][\"generated_text\"]\n",
        "            generated = generated.split(\"\\n\")[0]\n",
        "            gen_json = json.loads(generated)\n",
        "\n",
        "            entities = []\n",
        "            for span, head in gen_json[\"entities\"]:\n",
        "                entities.append(Entity(head, span))\n",
        "\n",
        "            triplets = []\n",
        "            for s, pred, o in gen_json[\"relations\"]:\n",
        "                triplets.append(Triplet(s, pred, o))\n",
        "\n",
        "            yield SceneGraph.new(entities=entities, triplets=triplets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00CusqmJKpGm"
      },
      "source": [
        "Despite the recent claims on the capabilities of open LLM, we found that the quality of the generated scene graphs was not good enough for our purposes, if not for some simple sentences. Furthermore, the inference time was too high to be able to preprocess the whole dataset in reasonable times with our limited computational resources. For example, on a single A100 GPU, the inference time for a single sentence was around 15 seconds, thus requiring more than 16 days to preprocess the whole dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D--OpdTBKpGn"
      },
      "source": [
        "For these reasons, we decided to use ChatGPT (`gpt-3.5-turbo`) through the APIs made available by OpenAI. Indeed, by making ChatGPT parse mutliple sentences (`10`) for each request, we were able to reduce the time required to preprocess the whole dataset to around 10 hours. The number of sentences per request was chosen sufficiently high to reduce the inference time (and the cost of the API calls) but not too high to avoid the model to start hallucinating.\n",
        "\n",
        "Even in this case, when generating the prompt, we had to tradeoff between the number of examples and the quality of the generated scene graphs. In particular, by increasing the number of examples, we were able to show the model a wider variety of sentences, thus improving the quality of the scene graphs. However, this also increased the inference time\n",
        "and the cost of the API calls. At the end, we decided to use 7 examples that showed many typical constructions of the sentences in the dataset (for a short description of the reasons why we chose these examples, see the following code snippet)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oihs7pnpKpGn"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "import enum\n",
        "import os\n",
        "from typing import Any\n",
        "\n",
        "import openai\n",
        "\n",
        "from deepsight.data.structs import SceneGraph\n",
        "\n",
        "\n",
        "class GPTModel(enum.Enum):\n",
        "    GPT3_5 = \"gpt-3.5-turbo\"\n",
        "    GPT4 = \"gpt-4\"\n",
        "\n",
        "    def openai_model(self) -> str:\n",
        "        return self.value\n",
        "\n",
        "\n",
        "class SceneGraphParser:\n",
        "    def __init__(\n",
        "        self,\n",
        "        api_key: str | None = None,\n",
        "        model: GPTModel = GPTModel.GPT3_5,\n",
        "        temperature: float = 0.2,\n",
        "    ) -> None:\n",
        "        \"\"\"Initializes the parser with the given parameters.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        api_key : str, optional\n",
        "            OpenAI API key. If not provided, the token will be read from the\n",
        "            environment variable OPENAI_API_KEY.\n",
        "        model : GPTModel, optional\n",
        "            The GPT model to use. Defaults to GPTModel.GPT3_5.\n",
        "        temperature : float, optional\n",
        "            The temperature to use when sampling from the model. Should be between\n",
        "            0 and 2, where higher values will make the output more random, while\n",
        "            lower values will make the output more focused and deterministic.\n",
        "            Defaults to 0.2.\n",
        "        \"\"\"\n",
        "        if api_key is None:\n",
        "            api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "            if api_key is None:\n",
        "                raise ValueError(\n",
        "                    \"No OpenAI API key provided. Please provide a key or set the \"\n",
        "                    \"environment variable OPENAI_API_KEY.\"\n",
        "                )\n",
        "\n",
        "        openai.api_key = api_key\n",
        "        self.model = model\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def _build_requests(self, captions: list[str]) -> str:\n",
        "        output = \"\"\n",
        "        for caption in captions:\n",
        "            output += f\"<caption>{caption}</caption>\\n\"\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _build_examples(self, graphs: list[SceneGraph]) -> str:\n",
        "        output = \"\"\n",
        "        for graph in graphs:\n",
        "            output += f\"<json>{graph.to_dict()}</json>\\n\"\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _match_entity(self, entity: str, entities: list[dict[str, Any]]) -> int | None:\n",
        "        \"\"\"Matches the given entity to an entity in the list of entities.\"\"\"\n",
        "\n",
        "        for idx, ent in enumerate(entities):\n",
        "            if entity in ent[\"phrase\"]:\n",
        "                return idx\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _get_entity_index(\n",
        "        self, entity: str | int | None, entities: list[dict[str, Any]]\n",
        "    ) -> int | None:\n",
        "        if entity is None:\n",
        "            return None\n",
        "\n",
        "        entity_idx: int\n",
        "        if isinstance(entity, int):\n",
        "            entity_idx = entity\n",
        "        elif entity.isdigit():\n",
        "            entity_idx = int(entity)\n",
        "        else:\n",
        "            idx = self._match_entity(entity, entities)\n",
        "            if idx is None:\n",
        "                entities.append({\"noun\": entity, \"phrase\": entity})\n",
        "                entity_idx = len(entities) - 1\n",
        "            else:\n",
        "                entity_idx = idx\n",
        "\n",
        "        if entity_idx >= len(entities):\n",
        "            return None\n",
        "\n",
        "        return entity_idx\n",
        "\n",
        "    def _postprocess(self, output: dict[str, Any]) -> SceneGraph | None:\n",
        "        entities = output.get(\"entities\", [])\n",
        "        if len(entities) == 0:\n",
        "            return None\n",
        "\n",
        "        triplets = output.get(\"triplets\", [])\n",
        "        new_triplets = []\n",
        "        for triplet in triplets:\n",
        "            subj = self._get_entity_index(triplet.get(\"subject\"), entities)\n",
        "            obj = self._get_entity_index(triplet.get(\"object\"), entities)\n",
        "\n",
        "            match (subj, obj):\n",
        "                case (None, None):\n",
        "                    continue\n",
        "                case (None, obj):\n",
        "                    entities[obj][\"phrase\"] = (\n",
        "                        entities[obj][\"phrase\"] + \" \" + triplet[\"relation\"]\n",
        "                    )\n",
        "                case (subj, None):\n",
        "                    entities[subj][\"phrase\"] = (\n",
        "                        entities[subj][\"phrase\"] + \" \" + triplet[\"relation\"]\n",
        "                    )\n",
        "                case (subj, obj):\n",
        "                    new_triplets.append(\n",
        "                        {\n",
        "                            \"subject\": subj,\n",
        "                            \"object\": obj,\n",
        "                            \"relation\": triplet[\"relation\"],\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "        return SceneGraph.from_dict({\"entities\": entities, \"triplets\": new_triplets})\n",
        "\n",
        "    async def parse(\n",
        "        self, examples: list[tuple[str, SceneGraph]], captions: list[str]\n",
        "    ) -> list[tuple[str, SceneGraph | None]]:\n",
        "        \"\"\"Parses the given captions into scene graphs.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        examples : list[tuple[str, SceneGraph]]\n",
        "            A list of examples to use for the prompt. Each example is a tuple\n",
        "            consisting of a caption and the corresponding scene graph.\n",
        "        captions : list[str]\n",
        "            A list of captions to parse into scene graphs.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        list[tuple[str, SceneGraph | None]]\n",
        "            A list of tuples consisting of the original caption and the parsed\n",
        "            scene graph. If the parsing fails due to formatting issues, the scene\n",
        "            graph will be `None`.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        RuntimeError\n",
        "            If the parsing fails.\n",
        "        openai.error.OpenAIError\n",
        "            The error returned by the OpenAI API.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            res = await openai.ChatCompletion.acreate(\n",
        "                model=self.model.openai_model(),\n",
        "                temperature=self.temperature,\n",
        "                n=1,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system},\n",
        "                    { # we add the example captions as previous requests from the user\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": self._build_requests([cap for cap, _ in examples]),\n",
        "                    },\n",
        "                    { # we add the corresponding scene graphs as previous responses from the assistant\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": self._build_examples(\n",
        "                            [graph for _, graph in examples]\n",
        "                        ),\n",
        "                    },\n",
        "                    { # we add the captions to parse as the next request from the user\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": self._build_requests(captions),\n",
        "                    },\n",
        "                ],\n",
        "            )\n",
        "        except openai.error.OpenAIError as e:\n",
        "            raise e\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Input: {captions}\") from e\n",
        "\n",
        "        response: str = res[\"choices\"][0][\"message\"][\"content\"]\n",
        "        outputs = response.split(\"\\n\")\n",
        "\n",
        "        results: list[tuple[str, SceneGraph | None]] = []\n",
        "        for caption, output in zip(captions, outputs):\n",
        "            start = output.find(\"{\")\n",
        "            end = output.rfind(\"}\")\n",
        "            output = output[start : end + 1]\n",
        "\n",
        "            try:\n",
        "                output_dict = ast.literal_eval(output)\n",
        "                graph = self._postprocess(output_dict)\n",
        "                if graph is not None:\n",
        "                    results.append((caption, graph))\n",
        "                else:\n",
        "                    results.append((caption, None))\n",
        "            except Exception as e:\n",
        "                results.append((caption, None))\n",
        "                print(f\"Input: {caption} | Output: {output}\")\n",
        "                print(f\"Exception: {e}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "# this is the firt part of the prompt\n",
        "# after this, the examples are added\n",
        "system = \"\"\"\\\n",
        "You will be provided with a set of captions each describing a region in an image. \\\n",
        "For each region, first identify the entities, like people, objects or places, present in the region Specify both a single noun and a phrase that describes the entity. \\\n",
        "Then, identify the triplets of subject, relation and object that describe the relationships between the entities. \\\n",
        "\"\"\"  # noqa: E501\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3grDfFuUKpGo"
      },
      "outputs": [],
      "source": [
        "# This example is useful to show the model that the attributes of an entity can be non adjacent to the entity itself.\n",
        "# Furthermore, it shows that the location of an entity in the scene should be considered an attribute of the entity.\n",
        "example1 = (\n",
        "    \"There is a truck covered in snow farthest from the right\",\n",
        "    SceneGraph.new(\n",
        "        entities=[\n",
        "            Entity(\"truck\", \"a truck farthest from the right\"),\n",
        "            Entity(\"snow\", \"snow\"),\n",
        "        ],\n",
        "        triplets=[\n",
        "            Triplet(0, \"covered in\", 1),\n",
        "        ],\n",
        "    ),\n",
        ")\n",
        "\n",
        "# This example shows that the attributes of an entity do not need to be adjectives or adverbial phrases,\n",
        "# but can also relative clauses.\n",
        "example2 = (\n",
        "    \"A placemat is empty behind a placemat that is full\",\n",
        "    SceneGraph.new(\n",
        "        entities=[\n",
        "            Entity(\"placemat\", \"an empty placemat\"),\n",
        "            Entity(\"placemat\", \"a full placemat\"),\n",
        "        ],\n",
        "        triplets=[\n",
        "            Triplet(0, \"behind\", 1),\n",
        "        ],\n",
        "    ),\n",
        ")\n",
        "\n",
        "# This example condenses in one sentence the information provided by the two previous examples.\n",
        "example3 = (\n",
        "    \"the chair not being used in the background, perpendicular to the viewer\",\n",
        "    SceneGraph.new(\n",
        "        entities=[\n",
        "            Entity(\"chair\", \"the chair not being used in the background\"),\n",
        "            Entity(\"viewer\", \"the viewer\"),\n",
        "        ],\n",
        "        triplets=[\n",
        "            Triplet(0, \"perpendicular to\", 1),\n",
        "        ],\n",
        "    ),\n",
        ")\n",
        "\n",
        "# This example was added because in the dataset we found many descriptions of this form,\n",
        "# \"the book with the title X\" and we found that the model was not able to parse them.\n",
        "example4 = (\n",
        "    \"A double decker bus with the wording The Ghost Bus Tours.com on the side.\",\n",
        "    SceneGraph.new(\n",
        "        entities=[\n",
        "            Entity(\"bus\", \"a double decker bus\"),\n",
        "            Entity(\"wording\", \"the wording The Ghost Bus Tours.com\"),\n",
        "            Entity(\"side\", \"the side\"),\n",
        "        ],\n",
        "        triplets=[\n",
        "            Triplet(0, \"with\", 1),\n",
        "            Triplet(1, \"on\", 2),\n",
        "        ],\n",
        "    ),\n",
        ")\n",
        "\n",
        "# This example shows a quite complex sentence, with multiple entities and relations.\n",
        "# In particular, it shows the model that a pronoun should not be parsed as a new entity, but as a reference to an existing entity,\n",
        "# thus all relations involving the pronoun should be between the subject/object of the relation and the entity the pronoun refers to.\n",
        "# Furthemore, it shows that thr subject of the caption does not need to be the subject of all relations, but can also be the object of some relations.\n",
        "example5 = (\n",
        "    \"a man stands majestically on his skis on a snow covered area with 2 other people \"\n",
        "    + \"behind him in the distance\",\n",
        "    SceneGraph.new(\n",
        "        entities=[\n",
        "            Entity(\"man\", \"a man\"),\n",
        "            Entity(\"ski\", \"his skis\"),\n",
        "            Entity(\"snow area\", \"a snow covered area\"),\n",
        "            Entity(\"people\", \"2 other people\"),\n",
        "        ],\n",
        "        triplets=[\n",
        "            Triplet(0, \"stands on\", 1),\n",
        "            Triplet(0, \"on\", 2),\n",
        "            Triplet(3, \"behind\", 0),\n",
        "        ],\n",
        "    ),\n",
        ")\n",
        "\n",
        "# This example shows a quite simple and linear sentence, similar to many of the sentences in the dataset.\n",
        "# In particular, it shows the model that the expression \"<subject> in <clothing>\" (largely present in the dataset)\n",
        "# should be parsed as two different entities, one for the subject and one for the clothing (in a relation) and not as a single entity.\n",
        "example6 = (\n",
        "    \"Woman in white shirt looking down at laptop computer and \" + \"holding a glass\",\n",
        "    SceneGraph.new(\n",
        "        entities=[\n",
        "            Entity(\"woman\", \"woman\"),\n",
        "            Entity(\"shirt\", \"white shirt\"),\n",
        "            Entity(\"computer\", \"laptop computer\"),\n",
        "            Entity(\"glass\", \"a glass\"),\n",
        "        ],\n",
        "        triplets=[\n",
        "            Triplet(0, \"in\", 1),\n",
        "            Triplet(0, \"looking down at\", 2),\n",
        "            Triplet(0, \"holding\", 3),\n",
        "        ],\n",
        "    ),\n",
        ")\n",
        "\n",
        "# This simple example was added because we found that, based on the previous examples,\n",
        "# the model still did not correctly parsed the location of an entity as an attribute of the entity.\n",
        "example7 = (\n",
        "    \"Woman on the right\",\n",
        "    SceneGraph.new(entities=[Entity(\"woman\", \"woman on the right\")], triplets=[]),\n",
        ")\n",
        "\n",
        "examples = [example1, example2, example3, example4, example5, example6, example7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8eU1lLGKpGo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "a98d9ab5-d0a7-4fee-fee2-5416f3bedaa8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-63bcbdecedca>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The girl approaching the table while holding a glass\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mgpt_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# if the parsing was successful\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-91c66e472ec9>\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, examples, captions)\u001b[0m\n\u001b[1;32m    183\u001b[0m             )\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpenAIError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Input: {captions}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-91c66e472ec9>\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, examples, captions)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             res = await openai.ChatCompletion.acreate(\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenai_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36macreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36macreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morganization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         )\n\u001b[0;32m--> 217\u001b[0;31m         response, _, api_key = await requestor.arequest(\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36marequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             )\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_async_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;32mawait\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__aexit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_async_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_warn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             return (\n\u001b[0;32m--> 726\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    727\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0;32mawait\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             )\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Incorrect API key provided: api key. You can find your API key at https://platform.openai.com/account/api-keys."
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "\n",
        "gpt_parser = SceneGraphParser(\"api key\")\n",
        "\n",
        "sentence = \"The girl approaching the table while holding a glass\"\n",
        "\n",
        "# note: this request will fail since the provided api key is not valid\n",
        "# if necessary, we can provide a temporary api key for testing\n",
        "res = await gpt_parser.parse(examples, [sentence])\n",
        "graph = res[0][1]\n",
        "if graph is not None: # if the parsing was successful\n",
        "    print(graph.entities())\n",
        "    print(graph.triplets(None, True, False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCRA5QifKpGp"
      },
      "source": [
        "Some considerations on the quality of the generated scene graphs:\n",
        "1. Among the methods previously illustrated, the graph generated with ChatGPT are by far the best ones, but they are still far from perfect. In particular, we noticed that ChatGPT is inconsistent in the generated scene graphs, i.e., clauses with extremely similar structures (and thus corresponding scene graphs) are associated to different scene graphs. For example, spatial locations are not always treated as attributes of the entity they refer to, but they are sometimes parsed as spatial relations, despite the mutliple example in the prompt. Furthermore, we noticed that sometimes the text associated to a relation is also associated to one of the entities involved.\n",
        "2. The textual descriptions of the parsed entities and relations are strongly based on the words present in the input sentence. For example, in a sentence like *\"the woman in a green shirt\"*, the relation is described simply using the preposition `in` like it is in the sentence. While this is not wrong, a better description of the relation would be `wearing`. Similarly, in the sentence *\"the zebra walking with its young one*\", the detected entities are `the zebra` and `its young one` which is not wrong, but a better description would be `the zebra` and `the young zebra`. When generating the prompt, we tried to teach the model to not necessarily use the words present in the sentence, but the model started to hallucinate thus compromising the quality of the generated scene graphs. <br>\n",
        "We noticed similar results when we tried to make the model generate different relations based on whether the subject of the relation in the scene graph is also the subject of the relation in the sentence. For example, given a sentence like *\"the girl looking at the table\"*, the generated scene graph would be a graph with two nodes (`the girl`, `the table`) and an undirected edge between the two representing the relation `looking at`. We tried to make the model create a directed scene graph with the relation `looking at` from `the girl` to `the table` and the relation `being looked at` from `the table` to `the girl`, but the model started to hallucinate. <br>\n",
        "Notice that GPT4 is instead able to generate such more complex scene graphs, showing that it has a better \"understading\" of the content of the sentence. Unfortunately, the GPT4 API were not openly available at the time of the project and their cost is 20x higher than the cost of the ChatGPT API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yetqM53QKpGp"
      },
      "source": [
        "### Architecture modules\n",
        "\n",
        "<img src=\"https://github.com/FrancescoGentile/visgator/blob/deepsight/docs/img/sgg.png?raw=true\" width=\"800px\"/>\n",
        "\n",
        "The developed framework decomposes the model architecture (here called `Pipeline`) into four main modules:\n",
        "- `PreProcessor`: module used to transform the input data before feeding it to the model. Notice that such module at training time takes in input also the target data since it may be necessary for implementing some training strategies like denoising bounding box coordinates ([Li et al. 2022](https://arxiv.org/abs/2203.01305), [Zhang et al. 2022](https://arxiv.org/abs/2203.03605)).\n",
        "- `Model`: this is the core of the pipeline and it is responsible for all the main computations.\n",
        "- `PostProcessor`: module used to transform the output of the model into the same format of the target data.\n",
        "- `Criterion`: module used to compute the loss between the output of the model and the target data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6vt3VkiKpGp"
      },
      "source": [
        "#### PreProcessor\n",
        "\n",
        "In the preprocessor, we perform two main preprocessing steps. First, we resize each image such that its shortest size is 800px keeping its original aspect ratio. If by doing so, the longest size is longer than 1333px, then we resize the image such that its longest size is 1333px (still keeping the aspect ratio). This is the same strategy used by Grounding DINO. The we standardize the image using the channels mean and standard deviation used by CLIP.\n",
        "\n",
        "The preprocessor is also the step where the scene graph is generated. Since generating the scene graph using ChatGPT has a high latency (more than 10 seconds), it is not feasible to perform this step in real time. Thus, we preprocessed the whole dataset before training the model. The generated scene graphs are then stored in a json file that is loaded by the preprocessor at initialization.\n",
        "\n",
        "Since the candidate bounding boxes are generated from the embeddings of the subject nodes, in case the generated scene graph has more than one connected component, we remove from the scene graph all components except the one containing the subject entity. In fact, since during the `Graph Attention` step, each node can pass directly (or indirectly thorugh its neighbours) messages only with the nodes in the same connected component, nodes in different connected components would not be able to exchange messages and thus influence the embeddings of the subject nodes. Keeping them would thus be useless."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1fM_2OVKpGq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0724d0e6-8c37-4660-ed73-fa1e872a4422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
            "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
            "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from typing import Any\n",
        "\n",
        "from deepsight.data.structs import Batch, RECInput, RECOutput, SceneGraph\n",
        "from deepsight.data.transformations import Compose, Resize, Standardize\n",
        "from deepsight.modeling.pipeline import PreProcessor as _PreProcessor\n",
        "from deepsight.utils.torch import Batched3DTensors\n",
        "\n",
        "from projects.sgg.modeling import PreprocessorConfig\n",
        "from projects.sgg.modeling._structs import ModelInput\n",
        "\n",
        "\n",
        "class PreProcessor(_PreProcessor[RECInput, RECOutput, ModelInput]):\n",
        "    def __init__(self, config: PreprocessorConfig) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self._preparsed: dict[str, dict[str, Any]] = {}\n",
        "        if config.file is not None:\n",
        "            with config.file.open(\"r\") as f:\n",
        "                self._preparsed = json.load(f)\n",
        "\n",
        "        # self._parser = gpt.SceneGraphParser(config.token)\n",
        "\n",
        "        self._transform = Compose(\n",
        "            [\n",
        "                Resize(config.side, max_size=config.max_side, p=1.0),\n",
        "                Standardize(config.mean, config.std, p=1.0),\n",
        "            ],\n",
        "            p=1.0,\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        inputs: Batch[RECInput],\n",
        "        targets: Batch[RECOutput] | None,\n",
        "    ) -> ModelInput:\n",
        "        graphs = []\n",
        "        for inp in inputs:\n",
        "            if inp.description in self._preparsed:\n",
        "                scene_graph = SceneGraph.from_dict(self._preparsed[inp.description])\n",
        "                # remove all nodes not connected to the root node\n",
        "                # since they will never pass messages to the root node\n",
        "                # (directly or through other nodes)\n",
        "                scene_graph = scene_graph.node_connected_component(0)\n",
        "                graphs.append(scene_graph)\n",
        "            else:\n",
        "                # we currently do not support real-time scene graph generation\n",
        "                raise NotImplementedError\n",
        "\n",
        "        return ModelInput(\n",
        "            images=[i.image for i in inputs],\n",
        "            features=Batched3DTensors.from_list(\n",
        "                [self._transform(inp.image)[0].to_tensor().data for inp in inputs]\n",
        "            ),\n",
        "            captions=[i.description for i in inputs],\n",
        "            graphs=graphs,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr3hIYa2KpGr"
      },
      "source": [
        "#### Model\n",
        "\n",
        "As previously said, the model consists of four main components:\n",
        "1. Vision Encoder\n",
        "2. Text Encoder\n",
        "3. Object Detector\n",
        "4. Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBYLhS6aKpGr"
      },
      "source": [
        "##### Vision Encoder\n",
        "\n",
        "The vision encoder used is a modified version of the ViT encoder used by CLIP. In particular, the original CLIP vision encoder returns a simgle embedding for each image. In our case, we need it to return a feature map for each image to allow each query to attend to different parts of the image. Thus, we modified the cLIP implementation by removing the attention-based pooling and taking the patch embeddings of the last layer as the feature map. Similarly to OwlViT ([Minderer et al. 2022](https://arxiv.org/abs/2205.06230)), we further multiply each patch embedding with the class token and we apply a layer norm. The authors of OwlViT state that this last operation improves the performance of the model, but no explanation if given. Probably, this allow each patch to encode information of the whole image and not only of a specific region. Since the feature dimension of each patch is 768, we also apply a final linear projection to reduce the dimension to 256 (the same dimension used by DETR-like models). To avoid losing all the information of the discarded linear projection, we initialize the weights by applying PCA to the original weights (even though no significant improvement was observed with respect to random initialization).\n",
        "\n",
        "Another difference is that the original CLIP encoder resizes and center-crops each image to a 224x224 format. By resizing the image to such a small dimension, small details in the image may be lost, that may be important for the object detection task. Firthermore, by center cropping the image, some parts of the image thatmay contain some of the entities referred in the sentence may be cropped out. Thus, as previously said, we resize each image to a larger size and we do not apply cropping. This is similar to what is done by OwlViT. However, while OwlViT resizes each image to a fixed size, we keep the original aspect ratio and we use padding (with attention masking) to handle images of different sizes in the same batch. Similarly to OwlViT, we do not discard the learned positional embeddings of CLIP, but we simply interpolate them to the new image size.\n",
        "\n",
        "Note: We used the CLIP vision encoder based on ViT instead of ResNet-50 because we verified that when giving in input to the encoder images with a larger size than what the model was trained on, the final embeddings were more similar to the embeddings obtained by using the 224x224 image. Thus it seems that the ViT encoder is more robust to changes in the input size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xeh7JVxiKpGr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from jaxtyping import Float\n",
        "from sklearn.decomposition import PCA\n",
        "from torch import Tensor, nn\n",
        "from transformers.models.clip.modeling_clip import (\n",
        "    CLIPVisionEmbeddings,\n",
        "    CLIPVisionModelWithProjection,\n",
        ")\n",
        "\n",
        "from deepsight.utils.torch import Batched2DTensors, Batched3DTensors\n",
        "\n",
        "from deepsight.modeling.layers.clip._misc import Models\n",
        "\n",
        "\n",
        "class VisionEncoder(nn.Module):\n",
        "    \"\"\"A modified version of the CLIP [1]_ vision encoder.\n",
        "\n",
        "    There are two main differences:\n",
        "    - While the CLIP vision encoder returns a single vector representation for each\n",
        "    image by pooling the patch embeddings, this encoder returns a 2D feature map for\n",
        "    each image. Similarly to OwlViT [2]_, the 2D feature map is obtained by multiplying\n",
        "    the patches with the class token and applying a layer norm. Each patch is then\n",
        "    projected to the output dimension using a linear layer.\n",
        "    - While the CLIP vision encoder requires all images to be rescaled to the same\n",
        "    size (224x224 or 336x336), this encoder does not require images to be rescaled to\n",
        "    the same fixed size. Instead, the positional embeddings are interpolated to the size\n",
        "    of each image. This should improve the performance of the encoder on large images\n",
        "    with fine-grained details.\n",
        "\n",
        "    .. note::\n",
        "        The CLIP vision encoder has an output dimension of 512 that is double what is\n",
        "        used by most object detection models. Thus, if the specified `output_dim` is\n",
        "        not 512, the projection layer is replaced with a linear layer that has the\n",
        "        specified output dimension. The weights of the linear layer are initialized by\n",
        "        applying PCA to the weights of the original projection layer.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal,\n",
        "        S., Sastry, G., Askell, A., Mishkin, P., Clark, J. and Krueger, G., 2021, July.\n",
        "        Learning transferable visual models from natural language supervision.\n",
        "        In International conference on machine learning (pp. 8748-8763). PMLR.\n",
        "    .. [2] Minderer, M., Gritsenko, A., Stone, A., Neumann, M., Weissenborn, D.,\n",
        "        Dosovitskiy, A., Mahendran, A., Arnab, A., Dehghani, M., Shen, Z. and Wang, X.,\n",
        "        2022, October. Simple open-vocabulary object detection. In European Conference\n",
        "        on Computer Vision (pp. 728-755). Cham: Springer Nature Switzerland.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model: Models, output_dim: int) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        clip = CLIPVisionModelWithProjection.from_pretrained(model.weights())\n",
        "\n",
        "        vision = clip.vision_model\n",
        "        self.embeddings = VisionEmbeddings(vision.embeddings)\n",
        "        self.pre_layernorm = vision.pre_layrnorm\n",
        "        self.encoder = vision.encoder\n",
        "        self.post_layernorm = vision.post_layernorm\n",
        "\n",
        "        self.last_layernorm = nn.LayerNorm(clip.config.hidden_size)\n",
        "\n",
        "        projection = clip.visual_projection\n",
        "        if projection.out_features != output_dim:\n",
        "            weights = projection.weight.transpose(0, 1).detach().numpy()\n",
        "            weights = PCA(output_dim).fit_transform(weights)\n",
        "            self.projection = nn.Linear(\n",
        "                in_features=projection.in_features,\n",
        "                out_features=output_dim,\n",
        "                bias=False,\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                self.projection.weight = nn.Parameter(\n",
        "                    torch.from_numpy(weights).transpose(0, 1)\n",
        "                )\n",
        "\n",
        "        else:\n",
        "            self.projection = projection\n",
        "\n",
        "    def _create_attention_mask(self, x: Batched2DTensors) -> Float[Tensor, \"B 1 L L\"]:\n",
        "        \"\"\"Creates an attention mask to mask out the padding tokens.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : Batched2DTensors\n",
        "            The input flattened image tensors.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Float[Tensor, \"B 1 L L\"]\n",
        "            The attention mask.\n",
        "        \"\"\"\n",
        "\n",
        "        mask = x.mask[:, None, None, :].expand(-1, 1, x.shape[1], -1)\n",
        "\n",
        "        dtype = x.tensor.dtype\n",
        "        attn_mask = torch.zeros_like(mask, dtype=dtype)\n",
        "        attn_mask.masked_fill_(mask, -torch.inf)\n",
        "\n",
        "        return attn_mask\n",
        "\n",
        "    def forward(self, images: Batched3DTensors) -> Batched3DTensors:\n",
        "        x, new_sizes = self.embeddings(images)  # (B, 1+HW, C)\n",
        "\n",
        "        attn_mask = self._create_attention_mask(x)\n",
        "\n",
        "        hidden: Tensor = self.pre_layernorm(x.tensor)\n",
        "        tmp = self.encoder(\n",
        "            inputs_embeds=hidden,\n",
        "            output_attentions=False,\n",
        "            output_hidden_states=False,\n",
        "            attention_mask=attn_mask,\n",
        "            return_dict=True,\n",
        "        )\n",
        "\n",
        "        hidden = tmp.last_hidden_state  # (B, 1+HW, C)\n",
        "        hidden = self.post_layernorm(hidden)\n",
        "\n",
        "        class_token = hidden[:, :1]  # (B, 1, C)\n",
        "        image_embeds = hidden[:, 1:]  # (B, HW, C)\n",
        "\n",
        "        out: Tensor = class_token * image_embeds\n",
        "        out = self.last_layernorm(out)\n",
        "\n",
        "        out = self.projection(out)  # (B, HW, D)\n",
        "\n",
        "        H = max(size[0] for size in new_sizes)\n",
        "        W = max(size[1] for size in new_sizes)\n",
        "\n",
        "        out = out.view(out.shape[0], H, W, -1).permute(0, 3, 1, 2)  # (B, D, H, W)\n",
        "\n",
        "        return Batched3DTensors(out, sizes=new_sizes)\n",
        "\n",
        "    def __call__(self, images: Batched3DTensors) -> Batched3DTensors:\n",
        "        return super().__call__(images)  # type: ignore\n",
        "\n",
        "\n",
        "class VisionEmbeddings(nn.Module):\n",
        "    \"\"\"A wrapper around the CLIP vision embeddings.\n",
        "\n",
        "    This wrapper allows the CLIP vision encoder to work with batches of images of\n",
        "    different sizes. To avoid discarding the learned positional embeddings, the\n",
        "    positional embeddings are interpolated to the size of each image.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embeddings: CLIPVisionEmbeddings) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embedding = embeddings.patch_embedding\n",
        "        self.class_embedding = embeddings.class_embedding\n",
        "\n",
        "        h, w = (int(embeddings.num_patches**0.5),) * 2\n",
        "        patch_pos_embedding = embeddings.position_embedding.weight.data[1:]\n",
        "        patch_pos_embedding = patch_pos_embedding.reshape(h, w, -1).permute(2, 0, 1)\n",
        "        class_pos_embedding = embeddings.position_embedding.weight.data[0]\n",
        "\n",
        "        self.patch_pos_embedding = nn.Parameter(patch_pos_embedding)\n",
        "        self.class_pos_embedding = nn.Parameter(class_pos_embedding)\n",
        "\n",
        "    def _compute_new_size(self, old_size: tuple[int, int]) -> tuple[int, int]:\n",
        "        \"\"\"Computes the new size of the image after patch embedding.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        old_size : tuple[int, int]\n",
        "            The size of the image before patch embedding.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple[int, int]\n",
        "            The size of the image after patch embedding.\n",
        "        \"\"\"\n",
        "\n",
        "        kh, kw = self.patch_embedding.kernel_size\n",
        "        sh, sw = self.patch_embedding.stride\n",
        "        ph, pw = self.patch_embedding.padding\n",
        "\n",
        "        H, W = old_size\n",
        "        h = (H + 2 * ph - kh) // sh + 1\n",
        "        w = (W + 2 * pw - kw) // sw + 1\n",
        "\n",
        "        return h, w\n",
        "\n",
        "    def forward(\n",
        "        self, images: Batched3DTensors\n",
        "    ) -> tuple[Batched2DTensors, list[tuple[int, int]]]:\n",
        "        B = len(images)\n",
        "        x: Tensor = self.patch_embedding(images.tensor)\n",
        "\n",
        "        new_sizes = []\n",
        "        patch_pos_emb = torch.zeros_like(x)  # (B, C, H, W)\n",
        "\n",
        "        for idx in range(len(x)):\n",
        "            h, w = self._compute_new_size(images.sizes[idx])\n",
        "            new_sizes.append((h, w))\n",
        "\n",
        "            # resize the positional embeddings to the new size of the image\n",
        "            emb = F.interpolate(\n",
        "                self.patch_pos_embedding[None],\n",
        "                size=(h, w),\n",
        "                mode=\"bilinear\",\n",
        "                align_corners=False,\n",
        "            )[0]\n",
        "\n",
        "            patch_pos_emb[idx, :, :h, :w] = emb\n",
        "\n",
        "        patch_pos_emb = patch_pos_emb.flatten(2).transpose(1, 2)  # (B, HW, C)\n",
        "        class_pos_emb = self.class_pos_embedding.expand(B, 1, -1)  # (B, 1, C)\n",
        "        pos_emb = torch.cat([class_pos_emb, patch_pos_emb], dim=1)  # (B, 1+HW, C)\n",
        "\n",
        "        class_token = self.class_embedding.expand(B, 1, -1)  # (B, 1, C)\n",
        "        x = x.flatten(2).transpose(1, 2)  # (B, HW, C)\n",
        "        x = torch.cat([class_token, x], dim=1)  # (B, 1+HW, C)\n",
        "\n",
        "        x = x + pos_emb\n",
        "\n",
        "        out = Batched2DTensors(x, sizes=[(1 + h * w) for h, w in new_sizes])\n",
        "\n",
        "        return out, new_sizes\n",
        "\n",
        "    def __call__(\n",
        "        self, images: Batched3DTensors\n",
        "    ) -> tuple[Batched2DTensors, list[tuple[int, int]]]:\n",
        "        return super().__call__(images)  # type: ignore\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO3ByYe8KpGs"
      },
      "source": [
        "##### Text Encoder\n",
        "\n",
        "The text encoder used is the CLIP text encoder. Thus, differently from the vision encoder, a sentence consisting of many words is encoded into a single embedding and not into a sequence of embeddings.\n",
        "\n",
        "The only difference is that, to make the output dimension match the feature dimension of each visual patch, we change the last linear projection with a new one that reduces the dimension to 256. As for the vision encoder, we initialize the weights of this linear projection by applying PCA to the original weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlGvCWFQKpGs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from jaxtyping import Float\n",
        "from sklearn.decomposition import PCA\n",
        "from torch import Tensor, nn\n",
        "from transformers.models.clip.modeling_clip import (\n",
        "    CLIPTextModelWithProjection,\n",
        ")\n",
        "from transformers.models.clip.processing_clip import CLIPProcessor\n",
        "\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    \"\"\"A wrapper around the CLIP [1]_ text encoder.\n",
        "\n",
        "    .. note::\n",
        "        To make the output dimension of the text encoder match the output dimension\n",
        "        of the vision encoder, if the specified `output_dim` is different from\n",
        "        the dimension of the text encoder's projection layer, the projection layer\n",
        "        is replaced with a new linear layer that has the specified output dimension.\n",
        "        The weights of the linear layer are initialized by applying PCA to the weights\n",
        "        of the original projection layer.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal,\n",
        "        S., Sastry, G., Askell, A., Mishkin, P., Clark, J. and Krueger, G., 2021, July.\n",
        "        Learning transferable visual models from natural language supervision.\n",
        "        In International conference on machine learning (pp. 8748-8763). PMLR.\"\"\"\n",
        "\n",
        "    def __init__(self, model: Models, output_dim: int) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self._dummy = nn.Parameter(torch.empty(0))\n",
        "\n",
        "        self.processor = CLIPProcessor.from_pretrained(model.weights())\n",
        "        self.transformer = CLIPTextModelWithProjection.from_pretrained(model.weights())\n",
        "\n",
        "        projection = self.transformer.text_projection\n",
        "        if projection.out_features != output_dim:\n",
        "            weights = projection.weight.transpose(0, 1).detach().numpy()\n",
        "            weights = PCA(output_dim).fit_transform(weights)\n",
        "            self.projection = nn.Linear(\n",
        "                in_features=projection.in_features,\n",
        "                out_features=output_dim,\n",
        "                bias=False,\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                self.projection.weight = nn.Parameter(\n",
        "                    torch.from_numpy(weights).transpose(0, 1)\n",
        "                )\n",
        "\n",
        "    def forward(self, text: list[str]) -> Float[Tensor, \"N D\"]:\n",
        "        inputs = self.processor(text=text, return_tensors=\"pt\", padding=True)\n",
        "        input_ids = inputs[\"input_ids\"].to(self._dummy.device)\n",
        "        attention_mask = inputs[\"attention_mask\"].to(self._dummy.device)\n",
        "\n",
        "        x = self.transformer(\n",
        "            input_ids=input_ids, attention_mask=attention_mask, return_dict=True\n",
        "        )\n",
        "        text_embeds = x.text_embeds  # (N, D)\n",
        "        out: Tensor = self.projection(text_embeds)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __call__(self, text: list[str]) -> Float[Tensor, \"N D\"]:\n",
        "        \"\"\"Encodes each text in the batch into a vector.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        text : list[str]\n",
        "            A list of texts to encode.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Float[Tensor, \"N D\"]\n",
        "            Tensor of shape (N, D) where N is the number of texts in the batch and D\n",
        "            is the output dimension of the text encoder.\n",
        "        \"\"\"\n",
        "\n",
        "        return super().__call__(text)  # type: ignore\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKKC2ASLKpGt"
      },
      "source": [
        "##### OwlViT\n",
        "\n",
        "As open-set object detector we decided to use OwlVit ([Minderer et al. 2022](https://arxiv.org/abs/2205.06230)) given its not poor performances (both in terms of inference speed and localization capabilities) and simplicity. Indeed, OwlViT is based on a pair of vision and text encoders that are pretrained on a contrastive image-text task, exactly like CLIP. Then, such architecture is finetuned on object detection by adding a simple regression head on top of the vision encoder. In particular, each patch (outputted by the vision encoder) is given in input to the regression head to predict the bounding boxes offsets with repsect to the patch position. The similarity between each patch and the text embeddings of the entities to detect is computed to obtain the confidence score that the patch contains the entity. A threshold is then applied to the confidence scores to obtain the final set of bounding boxes.\n",
        "\n",
        "Regarding the implementation, we used the one provided by [HuggingFace](https://huggingface.co/docs/transformers/model_doc/owlvit) with a modification to handle the case in which a given entity is not detected. In particular, if no instance of an entity is detected with a confidence score higher than the threshold, we select the __k__ most confident bounding boxes for that entity even if their confidence score is lower than the threshold. Even if the estimated locations of an entity are not accurate, we consider this approach better than not detecting the entity at all. Indeed, if the input scene graph is (`the girl` -- `approaching` --> `the table`), if no table is detected, the new graph will consist only of nodes associated to the entity `the girl`, thus missing the information that `the girl` that needs to be found is the one that is approaching `the table`.\n",
        "\n",
        "Other than OwlViT we also tried to use Grounding DINO as object detector. However, despite the better performance with respect to OwlViT, the inference speed was too slow making the training time for a single epoch more than double."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_pLdvE2KpGu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from jaxtyping import Float\n",
        "from torch import Tensor, nn\n",
        "from transformers import OwlViTForObjectDetection, OwlViTProcessor\n",
        "\n",
        "from deepsight.data.structs import (\n",
        "    Batch,\n",
        "    BoundingBoxes,\n",
        "    BoundingBoxFormat,\n",
        "    ODInput,\n",
        "    ODOutput,\n",
        ")\n",
        "\n",
        "\n",
        "class OwlViT(nn.Module):\n",
        "    \"\"\"Wrapper around the OwlViT model for open-set detection.\n",
        "\n",
        "    With respect to the original OwlViT model, this wrapper adds the possibility\n",
        "    to return the bounding boxes even when the confidence of the entity is below\n",
        "    a certain threshold.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, threshold: float, num_boxes: int | None = None) -> None:\n",
        "        \"\"\"Initializes the OwlViT model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        threshold : float\n",
        "            The threshold used to determine whether an entity is present in the\n",
        "            input image.\n",
        "        num_boxes : int | None\n",
        "            If not None, when an entity is not found with a confidence above the\n",
        "            `threshold`, the model will return the top `num_boxes` boxes with the\n",
        "            highest confidence scores for that entity. If None, the model will\n",
        "            return only the entities that are found with a confidence above the\n",
        "            `threshold`. Defaults to None.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self._threshold = threshold\n",
        "        self._num_boxes = num_boxes\n",
        "\n",
        "        self._dummy = nn.Parameter(torch.empty(0))\n",
        "\n",
        "        model_id = \"google/owlvit-base-patch32\"\n",
        "        processor = OwlViTProcessor.from_pretrained(model_id)\n",
        "        model = OwlViTForObjectDetection.from_pretrained(model_id)\n",
        "\n",
        "        self.processor = processor\n",
        "        self.owlvit = model.owlvit\n",
        "        self.class_head = model.class_head\n",
        "        self.box_head = model.box_head\n",
        "\n",
        "        self.layer_norm = model.layer_norm\n",
        "\n",
        "    def _get_boxes(\n",
        "        self, image_embeds: Float[Tensor, \"B L D\"]\n",
        "    ) -> Float[Tensor, \"B L 4\"]:\n",
        "        \"\"\"Returns for each patch the corresponding bounding box.\n",
        "\n",
        "        The bounding box associated to a patch is obtained by computing the coordinates offsets\n",
        "        with respect to the center of the patch using a simple regression head.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        image_embeds : Float[Tensor, \"B L D\"]\n",
        "            The image embeddings obtained from the OwlViT model. The shape is (B, L, D) where\n",
        "            B is the batch size, L is the number of patches and D is the dimension of the\n",
        "            embeddings.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Float[Tensor, \"B L 4\"]\n",
        "            The bounding boxes associated to each patch. The shape is (B, L, 4) where B is the\n",
        "            batch size, L is the number of patches and 4 are the normalized coordinates of the bounding box\n",
        "            in the format (center_x, center_y, width, height).\n",
        "        \"\"\"\n",
        "\n",
        "        L = image_embeds.shape[1]\n",
        "        side = int(L**0.5)\n",
        "        device = image_embeds.device\n",
        "        dtype = image_embeds.dtype\n",
        "\n",
        "        coords = torch.stack(\n",
        "            torch.meshgrid(\n",
        "                torch.arange(1, side + 1, device=device, dtype=dtype),\n",
        "                torch.arange(1, side + 1, device=device, dtype=dtype),\n",
        "                indexing=\"xy\",\n",
        "            ),\n",
        "            dim=-1,\n",
        "        )\n",
        "        coords = coords / side\n",
        "        coords = coords.view(L, 2)\n",
        "\n",
        "        coords = torch.clamp(coords, 0.0, 1.0)  # (L, 2)\n",
        "        coord_bias = torch.log(coords + 1e-4) - torch.log1p(-coords + 1e-4)\n",
        "\n",
        "        size = torch.full_like(coord_bias, 1.0 / side)  # (L, 2)\n",
        "        size_bias = torch.log(size + 1e-4) - torch.log1p(-size + 1e-4)\n",
        "\n",
        "        box_bias = torch.cat((coord_bias, size_bias), dim=-1)  # (L, 4)\n",
        "\n",
        "        pred_boxes: Tensor = self.box_head(image_embeds)  # (B, L, 4)\n",
        "        pred_boxes = pred_boxes + box_bias  # (B, L, 4)\n",
        "        pred_boxes = torch.sigmoid(pred_boxes)  # (B, L, 4)\n",
        "\n",
        "        return pred_boxes\n",
        "\n",
        "    def forward(self, inputs: Batch[ODInput]) -> Batch[ODOutput]:\n",
        "        images = [inp.image.to_pil().data for inp in inputs]\n",
        "\n",
        "        # Create list of list of entities and remove duplicates\n",
        "        entities: list[list[str]] = []\n",
        "        str_to_idx: list[dict[str, list[int]]] = []\n",
        "        for inp in inputs:\n",
        "            sample_entities = []\n",
        "            sample_str_to_idx = {}\n",
        "            for ent_idx, ent in enumerate(inp.entities):\n",
        "                # Since the OwlViT was trained by adding the prefix \"a photo of a\"\n",
        "                # to each category name, we do the same here.\n",
        "                # Indeed we find that the model is much more accurate and confident\n",
        "                # when the prefix is added.\n",
        "                ent = f\"a photo of a {ent}\"\n",
        "                if ent not in sample_str_to_idx:\n",
        "                    sample_str_to_idx[ent] = [ent_idx]\n",
        "                    sample_entities.append(ent)\n",
        "                else:\n",
        "                    sample_str_to_idx[ent].append(ent_idx)\n",
        "\n",
        "            entities.append(sample_entities)\n",
        "            str_to_idx.append(sample_str_to_idx)\n",
        "\n",
        "        B = len(images)\n",
        "        max_queries = max(len(ent) for ent in entities)\n",
        "\n",
        "        tmp = self.processor(\n",
        "            images=images, text=entities, return_tensors=\"pt\", truncation=True\n",
        "        )\n",
        "        pixel_values = tmp[\"pixel_values\"].to(self._dummy.device)\n",
        "        input_ids = tmp[\"input_ids\"].to(self._dummy.device)\n",
        "        attention_mask = tmp[\"attention_mask\"].to(self._dummy.device)\n",
        "\n",
        "        outputs = self.owlvit(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict=True,\n",
        "        )\n",
        "\n",
        "        image_tokens = outputs.vision_model_output[0]  # (B, 1+L, D)\n",
        "        image_tokens = self.owlvit.vision_model.post_layernorm(\n",
        "            image_tokens\n",
        "        )  # (B, 1+L, D)\n",
        "\n",
        "        class_token = image_tokens[:, :1, :]  # (B, 1, D)\n",
        "        image_patches = image_tokens[:, 1:, :]  # (B, L, D)\n",
        "        image_embeds = image_patches * class_token  # (B, L, D)\n",
        "        image_embeds = self.layer_norm(image_embeds)  # (B, L, D)\n",
        "\n",
        "        query_embeds = outputs[-4]  # (BQ, D) where BQ = B * max_queries\n",
        "        query_embeds = query_embeds.view(B, max_queries, -1)  # (B, Q, D)\n",
        "\n",
        "        query_mask = torch.zeros(\n",
        "            (B, max_queries),\n",
        "            dtype=torch.bool,\n",
        "            device=query_embeds.device,\n",
        "        )\n",
        "        for sample_idx, sample_entities in enumerate(entities):\n",
        "            query_mask[sample_idx, : len(sample_entities)] = True\n",
        "\n",
        "        # (B, L, Q) means that each image patch is compared to each query\n",
        "        # embedding, and the result is a scalar value representing the\n",
        "        # similarity between the two.\n",
        "        pred_logits, _ = self.class_head(image_embeds, query_embeds, query_mask)\n",
        "        pred_boxes = self._get_boxes(image_embeds)\n",
        "\n",
        "        probs, labels = pred_logits.max(dim=-1)  # (B, L)\n",
        "        if self._num_boxes is not None:\n",
        "            _, top_index_per_query = torch.topk(\n",
        "                pred_logits, self._num_boxes, dim=1\n",
        "            )  # (B, K, Q)\n",
        "\n",
        "        scores = probs.sigmoid()  # (B, L)\n",
        "\n",
        "        results = []\n",
        "        for sample_idx in range(B):\n",
        "            mask = scores[sample_idx] > self._threshold\n",
        "\n",
        "            sample_pred_boxes = pred_boxes[sample_idx, mask]  # (N, 4)\n",
        "            sample_pred_labels = labels[sample_idx, mask]  # (N,)\n",
        "            sample_pred_scores = scores[sample_idx, mask]  # (N,)\n",
        "\n",
        "            boxes_list: list[Tensor] = []\n",
        "            labels_list: list[int] = []\n",
        "            scores_list: list[Tensor] = []\n",
        "\n",
        "            for ent_idx, ent in enumerate(entities[sample_idx]):\n",
        "                indices = sample_pred_labels == ent_idx\n",
        "                num_found = indices.sum().item()\n",
        "                if num_found > 0:\n",
        "                    # entity found in image\n",
        "                    # add all duplicates to the list\n",
        "                    for j in str_to_idx[sample_idx][ent]:\n",
        "                        boxes_list.append(sample_pred_boxes[indices])\n",
        "                        labels_list.extend([j] * num_found)\n",
        "                        scores_list.append(sample_pred_scores[indices])\n",
        "                elif self._num_boxes is not None:\n",
        "                    # entity not found in image\n",
        "                    # add top K boxes for the entity\n",
        "                    topk_boxes = pred_boxes[\n",
        "                        sample_idx, top_index_per_query[sample_idx, :, ent_idx]\n",
        "                    ]\n",
        "\n",
        "                    topk_scores = scores[\n",
        "                        sample_idx, top_index_per_query[sample_idx, :, ent_idx]\n",
        "                    ]\n",
        "\n",
        "                    for j in str_to_idx[sample_idx][ent]:\n",
        "                        boxes_list.append(topk_boxes)\n",
        "                        labels_list.extend([j] * self._num_boxes)\n",
        "                        scores_list.append(topk_scores * self._num_boxes)\n",
        "\n",
        "            sample_labels = torch.tensor(\n",
        "                labels_list, dtype=torch.long, device=self._dummy.device\n",
        "            )\n",
        "\n",
        "            boxes = BoundingBoxes(\n",
        "                tensor=torch.cat(boxes_list, dim=0),\n",
        "                images_size=inputs[sample_idx].image.size,\n",
        "                format=BoundingBoxFormat.CXCYWH,\n",
        "                normalized=True,\n",
        "            )\n",
        "\n",
        "            sample_scores = torch.cat(scores_list, dim=0)\n",
        "\n",
        "            results.append(\n",
        "                ODOutput(\n",
        "                    boxes=boxes,\n",
        "                    entities=sample_labels,\n",
        "                    scores=sample_scores,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return Batch(results)\n",
        "\n",
        "    def __call__(self, inputs: Batch[ODInput]) -> Batch[ODOutput]:\n",
        "        \"\"\"Given a batch of images and entities to be detected, returns a list\n",
        "        of OSDOutput objects containing the bounding boxes of the detected entities.\n",
        "\n",
        "        .. note::\n",
        "            In case of duplicate entities for the same image, this implementation will\n",
        "            return the same bounding boxes for both entities. This is different from the\n",
        "            implementation of HuggingFace's OwlViT model which returns the bounding\n",
        "            boxes only for one of the entities (usually the first one).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        inputs : Batch[OSDInput]\n",
        "            A Batch object containing OSDInput objects containing the images and\n",
        "            entities.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Batch[OSDOutput]\n",
        "            A Batch object containing the OSDOutput objects containing the bounding\n",
        "            boxes.\n",
        "        \"\"\"\n",
        "\n",
        "        return super().__call__(inputs)  # type: ignore\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbbOlecwKpGw"
      },
      "source": [
        "##### Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loVp1O84KpGw"
      },
      "source": [
        "In DETR-like models, the decoder contains a self-attention step to make each query proposal attend to all the other query proposals allowing them to exchange information. Such information can be used for example to avoid two queries to be assigned to the same entity or to suggest that an entity may be present (for example, if a query is associated to a ball, then there may be a kid in the adjacent region). In our case, however an entity node does not need to attend to all the other entities but only to the ones with which it should have a relation according to the scene graph. However, we also add edges (whose features are initialized with learnable parameters) between all nodes associated to the same entity to avoid multiple nodes to focus on the same entity instance.\n",
        "\n",
        "By exchaning information with its neighbours, each node can determine whether the associated entity instance is the one referred in the sentence. For example, if the sentence were *\"the girl approaching the table\"*, the generated scene graph would be (`the girl` -- `approaching` -> `the table`). If two different instances of a girl are found in the image, we need to choose which instance is the right one. Suppose also that only one instance of table is detected. When passing messages with the `table` node, each `girl` node can determine whether it is the right one by checking whether the relation `approaching` exists between the two nodes.\n",
        "\n",
        "For these reasons, to update the graph features, we do not use the self-attention mechanism (that can be considered a [graph operation](https://thegradient.pub/transformers-are-graph-neural-networks/)), but we use a more traditional message passing graph neural network (MPNN). In particular, we decided to use Graph Attention ([Veličković et al. 2017](https://arxiv.org/abs/1710.10903), [Brody at el. 2021](https://arxiv.org/abs/2105.14491)), given its simplicity and the good results obtained in many tasks (there are more performant graph operations but since we operate on very small graphs the advantages should not be significant).\n",
        "\n",
        "In the second version of Graph Attention ([Brody at el. 2021](https://arxiv.org/abs/2105.14491)), the node features are updated as follows:\n",
        "1. the messages passed by the node's neighbours (usually, the one hop neighourhood) are collected;\n",
        "2. the messages are aggregated, each weighted by an attention coefficient that measures the relevance of the message for the node (to compute the relance, edge features can also be used if present);\n",
        "3. the current node features are combined with the aggregated messages to obtain the new node features.\n",
        "\n",
        "In formulas:\n",
        "$$ x_i' = \\alpha_{ii} \\theta x_i + \\sum_{j \\in N_i}{\\alpha_{ij} \\theta x_j} $$\n",
        "and the attention coefficients are computed as:\n",
        "$$ \\alpha_{ij} = \\frac{\\exp{(W_2^\\top \\textrm{LeakyReLU}(W_1 [x_i || x_j || e_{ij}]))}}{\\sum_{k \\in N_i \\cup \\{i\\}}{\\exp{(W_2^\\top \\textrm{LeakyReLU}(W_1 [x_i || x_k || e_{ik}]))}}} $$\n",
        "\n",
        "where $\\theta$, $W_1$ and $W_2$ are learnable parameters, $e_{ij}$ is the edge feature between node $i$ and $j$ and $||$ is the concatenation operator. Usually, multi-head attention is used, where different attention coefficients are computed for each head, thus obtaining different node features for each head. The final node features are obtained by concatenating the node features of each head.\n",
        "\n",
        "With respect to such formulation, we make two main modifications:\n",
        "1. we do not use $\\textrm{LeakyReLU}$ but we use $\\textrm{GELU}$ in the attention coefficients computation;\n",
        "2. we add a step to also update the edge features. In particular, we compute the edge features as:\n",
        "$$ e_{ij}' = W_3 \\textrm{GELU}(W_1 [x_i || x_j || e_{ij}]) $$\n",
        "that is we concatenate the endpoint nodes features and the edge features and we apply a MLP with one hidden layer to obtain the new edge features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQr0EC-KKpGx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch_scatter import scatter_add, scatter_softmax\n",
        "\n",
        "from deepsight.utils.torch import BatchedGraphs\n",
        "\n",
        "\n",
        "class GATConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        bias: bool = True,\n",
        "        dropout: float = 0.0,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})\"\n",
        "            )\n",
        "\n",
        "        head_dim = embed_dim // num_heads\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.first_node_proj = nn.Linear(embed_dim, embed_dim, bias)\n",
        "        self.second_node_proj = nn.Linear(embed_dim, embed_dim, bias)\n",
        "        self.edge_proj = nn.Linear(embed_dim, embed_dim, bias)\n",
        "        self.attn_proj = nn.Parameter(torch.randn(1, num_heads, head_dim))\n",
        "\n",
        "        self.node_out_proj = nn.Linear(embed_dim, embed_dim, bias)\n",
        "        self.edge_out_proj = nn.Linear(embed_dim, embed_dim, bias)\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        graphs: BatchedGraphs,\n",
        "        embeddings: BatchedGraphs | None = None,\n",
        "    ) -> BatchedGraphs:\n",
        "        nodes = graphs.nodes(None)\n",
        "        edges = graphs.edges(None)\n",
        "\n",
        "        N, _ = nodes.shape\n",
        "        E, _ = edges.shape\n",
        "        H = self.num_heads\n",
        "\n",
        "        query_nodes = nodes\n",
        "        query_edges = edges\n",
        "\n",
        "        if embeddings is not None:\n",
        "            query_nodes = query_nodes + embeddings.nodes(None)\n",
        "            query_edges = query_edges + embeddings.edges(None)\n",
        "\n",
        "        first_node = self.first_node_proj(query_nodes)[graphs.edge_indices[0]]\n",
        "        second_node = self.second_node_proj(query_nodes)[graphs.edge_indices[1]]\n",
        "        query_edges = self.edge_proj(query_edges)\n",
        "\n",
        "        hidden = first_node + second_node + query_edges\n",
        "        hidden = F.gelu(hidden)\n",
        "\n",
        "        hidden_head = hidden.view(E, H, -1)\n",
        "        presoftmax_alpha = (hidden_head * self.attn_proj).sum(dim=-1)  # (E, H)\n",
        "        alpha = scatter_softmax(presoftmax_alpha, graphs.edge_indices[0], dim=0)\n",
        "        alpha = self.attn_dropout(alpha)\n",
        "\n",
        "        new_edges = self.edge_out_proj(hidden)\n",
        "        values = nodes[graphs.edge_indices[1]] + new_edges\n",
        "        values = self.node_out_proj(values)\n",
        "        values = values.view(E, H, -1)\n",
        "        values = values * alpha.unsqueeze(-1)\n",
        "        new_nodes = scatter_add(values, graphs.edge_indices[0], dim=0)\n",
        "        new_nodes = new_nodes.view(N, -1)\n",
        "\n",
        "        return graphs.new_like(nodes=new_nodes, edges=new_edges)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        graphs: BatchedGraphs,\n",
        "        embeddings: BatchedGraphs | None = None,\n",
        "    ) -> BatchedGraphs:\n",
        "        return super().__call__(graphs, embeddings)  # type: ignore\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7Q8swVGKpGy"
      },
      "source": [
        "In DETR-like models, in the self attention step before projecting the box queries into the attention queries and keys, each box query embedding is summed with an embedding encoding the spatial position of the box ([Men et al. 2021](https://arxiv.org/abs/2108.06152), [Liu et al. 2022](https://arxiv.org/abs/2201.12329)). In our model, we do the same. In particular, the four coordinates of the bounding box are encoded using sinusoidal functions as in [Vaswani et al. 2017](https://arxiv.org/abs/1706.03762). Inspired by [Zhang et al. 2020](https://arxiv.org/abs/2012.06060), we also add to each edge feature a positional encoding that encodes the spatial relation between the two endpoint nodes. In particular, we use the same sinusoidal functions to encode the difference between the center of the two bounding boxes, their intersection over union (IoU) and their union.\n",
        "\n",
        "Since all the values (coordinates, IoU and union) are normalized between 0 and 1, following [Liu et al. 2022](https://arxiv.org/abs/2201.12329), we set the temperature for the sinusoidal functions to 20 instead of 10000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20dS03KsKpGy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from jaxtyping import Float\n",
        "from torch import Tensor, nn\n",
        "\n",
        "from deepsight.data.structs import BoundingBoxes\n",
        "\n",
        "\n",
        "class SinusoidalBoxEmbeddings(nn.Module):\n",
        "    \"\"\"Sinusoidal box embeddings.\n",
        "\n",
        "    This module computes sinusoidal embeddings for a set of bounding boxes. The\n",
        "    embeddings are computed by applying sinusoidal functions (as described in [1]_)\n",
        "    to the coordinates of the boxes and concatenating the results.\n",
        "\n",
        "    .. note::\n",
        "        Since such position embeddings are intended to be matched with the ones\n",
        "        computed for the feature maps, the coordinates embeddings are concatenated\n",
        "        in the following order: cx, cy, (w, h).\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    dim : int\n",
        "        The final embedding dimension. This is equal to the feature dimension used\n",
        "        for each coordinate times the number of coordinates used (2 or 4). If\n",
        "        `include_wh` is `True`, the dimension must be divisible by 4, otherwise it\n",
        "        must be even.\n",
        "    temperature : float\n",
        "        The temperature of the sinusoidal function. Defaults to `20`.\n",
        "    include_wh : bool\n",
        "        Whether to include the width and height of the boxes in the embeddings.\n",
        "        Defaults to `False`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        temperature: int = 20,\n",
        "        scale: float = 2 * torch.pi,\n",
        "        include_wh: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        if include_wh:\n",
        "            if dim % 4 != 0:\n",
        "                raise ValueError(f\"dim must be divisible by 4, got {dim}.\")\n",
        "        else:\n",
        "            if dim % 2 != 0:\n",
        "                raise ValueError(f\"dim must be even, got {dim}.\")\n",
        "\n",
        "        self.dim = dim\n",
        "        self.temperature = temperature\n",
        "        self.scale = scale\n",
        "        self.include_wh = include_wh\n",
        "\n",
        "    def forward(self, boxes: BoundingBoxes) -> Float[Tensor, \"... D\"]:\n",
        "        boxes = boxes.to_cxcywh().normalize()\n",
        "\n",
        "        dim = self.dim // 4 if self.include_wh else self.dim // 2  # D\n",
        "        temperature = self.temperature\n",
        "\n",
        "        dim_t = torch.arange(dim, dtype=torch.float32, device=boxes.device)  # (D,)\n",
        "        dim_t = temperature ** (2 * torch.div(dim_t, 2, rounding_mode=\"floor\") / dim)\n",
        "\n",
        "        if self.include_wh:\n",
        "            coords = boxes.tensor  # (..., 4)\n",
        "        else:\n",
        "            coords = boxes.tensor[..., :2]  # (..., 2)\n",
        "\n",
        "        pos = coords.unsqueeze(-1) * self.scale / dim_t\n",
        "        pos = torch.stack((pos[..., 0::2].sin(), pos[..., 1::2].cos()), dim=-1)\n",
        "        pos = pos.flatten(start_dim=-3)  # (..., D)\n",
        "\n",
        "        return pos\n",
        "\n",
        "    def __call__(self, boxes: BoundingBoxes) -> Float[Tensor, \"... D\"]:\n",
        "        \"\"\"Computes the embeddings for the given boxes.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        boxes : BoundingBoxes\n",
        "            The bounding boxes to compute the embeddings for. The bounding boxes\n",
        "            tensor can have any number of leading dimension.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Float[Tensor, \"... D\"]\n",
        "            The computed embeddings. The number of leading dimensions of the returned\n",
        "            tensor is equal to the number of leading dimensions of the bounding boxes\n",
        "            tensor. The last dimension is equal to the embedding dimension.\n",
        "        \"\"\"\n",
        "\n",
        "        return super().__call__(boxes)  # type: ignore\n",
        "\n",
        "\n",
        "\n",
        "class SinusoidalPairwiseBoxEmbeddings(nn.Module):\n",
        "    \"\"\"Sinusoidal embeddings for pairs of bounding boxes.\"\"\"\n",
        "\n",
        "    def __init__(self, dim: int, temperature: int = 20) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        if dim % 4 != 0:\n",
        "            raise ValueError(f\"dim must be divisible by 4, got {dim}.\")\n",
        "\n",
        "        self.dim = dim\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        first: BoundingBoxes,\n",
        "        second: BoundingBoxes,\n",
        "    ) -> Float[Tensor, \"... D\"]:\n",
        "        first = first.to_cxcywh().normalize()\n",
        "        second = second.to_cxcywh().normalize()\n",
        "\n",
        "        distance = first.tensor[..., :2] - second.tensor[..., :2]  # (..., 2)\n",
        "        iou = first.iou(second).unsqueeze(-1)  # (..., 1)\n",
        "        union = first.union(second).area().unsqueeze(-1)  # (..., 1)\n",
        "\n",
        "        coords = torch.cat((distance, iou, union), dim=-1)  # (..., 4)\n",
        "        coords = coords.unsqueeze(-1)  # (..., 4, 1)\n",
        "\n",
        "        dim = self.dim // 4  # D\n",
        "        temperature = self.temperature\n",
        "\n",
        "        dim_t = torch.arange(dim, dtype=torch.float32, device=coords.device)  # (D,)\n",
        "        dim_t = temperature ** (2 * torch.div(dim_t, 2, rounding_mode=\"floor\") / dim)\n",
        "\n",
        "        pos = coords / dim_t\n",
        "        pos = torch.stack((pos[..., 0::2].sin(), pos[..., 1::2].cos()), dim=-1)\n",
        "        pos = pos.flatten(start_dim=-3)  # (..., D)\n",
        "\n",
        "        return pos\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        first: BoundingBoxes,\n",
        "        second: BoundingBoxes,\n",
        "    ) -> Float[Tensor, \"... D\"]:\n",
        "        return super().__call__(first, second)  # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlFSU4YLKpG0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from jaxtyping import Bool, Float\n",
        "from torch import Tensor, nn\n",
        "\n",
        "from deepsight.data.structs import BoundingBoxes\n",
        "\n",
        "\n",
        "class GaussianHeatmaps(nn.Module):\n",
        "    \"\"\"Gaussian heatmaps.\n",
        "\n",
        "    This module computes a Gaussian heatmap for a set of bounding boxes.\n",
        "    The gaussian is centered at the center of the bounding box and its standard\n",
        "    devation in each direction is equal to the corresponding side of the bounding\n",
        "    box.\n",
        "\n",
        "    These heatmaps can be used to bias the attention of each query to a specific region\n",
        "    of the image during the cross-attention step of DETR-like models. See [1]_ for more\n",
        "    details.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    beta : float\n",
        "        A scaling factor for the gaussian standard deviation. The smaller the value,\n",
        "        the more concentrated the gaussian will be around the center of the bounding\n",
        "        box. Defaults to 1.0.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] Gao, P., Zheng, M., Wang, X., Dai, J. and Li, H., 2021. Fast convergence of\n",
        "        detr with spatially modulated co-attention. In Proceedings of the IEEE/CVF\n",
        "        international conference on computer vision (pp. 3621-3630).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, beta: float = 1.0) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.beta = beta\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        boxes: BoundingBoxes,\n",
        "        mask: Bool[Tensor, \"... H W\"],\n",
        "    ) -> Float[Tensor, \"... H W\"]:\n",
        "        boxes = boxes.to_cxcywh().normalize()\n",
        "\n",
        "        mean = boxes.tensor[..., :2]  # (..., 2)\n",
        "        std = boxes.tensor[..., 2:]  # (..., 2)\n",
        "\n",
        "        not_mask = ~mask\n",
        "        y_coords = not_mask.cumsum(dim=-2, dtype=torch.float32)  # (..., H, W)\n",
        "        x_coords = not_mask.cumsum(dim=-1, dtype=torch.float32)  # (..., H, W)\n",
        "\n",
        "        eps = torch.finfo(torch.float32).eps\n",
        "        y_coords = y_coords / (y_coords[..., -1:, :] + eps)\n",
        "        x_coords = x_coords / (x_coords[..., -1:] + eps)\n",
        "\n",
        "        y = (y_coords - mean[..., 1, None, None]) ** 2\n",
        "        y = y / (self.beta * (std[..., 1, None, None] ** 2))\n",
        "\n",
        "        x = (x_coords - mean[..., 0, None, None]) ** 2\n",
        "        x = x / (self.beta * (std[..., 0, None, None] ** 2))\n",
        "\n",
        "        out: Tensor = torch.exp(-(x + y))  # (..., H, W)\n",
        "        out.masked_fill_(mask, 0.0)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        boxes: BoundingBoxes,\n",
        "        mask: Bool[Tensor, \"... H W\"],\n",
        "    ) -> Float[Tensor, \"... H W\"]:\n",
        "        \"\"\"Computes log-Gaussian heatmaps.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        boxes : BoundingBoxes\n",
        "            The bounding boxes for which to compute the heatmaps. The bounding boxes\n",
        "            tensor can have any number of leading dimensions.\n",
        "        mask : Bool[Tensor, \"... H W\"]\n",
        "            A boolean mask indicating which pixels in the heatmaps should be considered\n",
        "            as padding, i.e. which pixels are outside the image. The mask tensor must\n",
        "            have the same number of leading dimensions as the bounding boxes tensor.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Float[Tensor, \"... H W\"]\n",
        "            The computed heatmaps. The number of leading dimensions of the returned\n",
        "            tensor is equal to the number of leading dimensions of the bounding boxes\n",
        "            tensor. The last two dimensions are equal to the height and width of the\n",
        "            heatmaps, respectively.\n",
        "        \"\"\"\n",
        "\n",
        "        return super().__call__(boxes, mask)  # type: ignore\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GuLdlZ-KpG0"
      },
      "source": [
        "The cross-attention operation in the decoder is not different from the one used in DETR-like models, that is the nodes and edges of the graph returned by the last decoder layer are used as queries, while the visual features outputted by the encoder are used to compute the keys and values. This allows each node to detect whether in the associated region there is the corresponding entity and allows each edge to detect whether the relation is present between the two endpoint nodes.\n",
        "\n",
        "Recent works ([Gao et al. 2021](https://arxiv.org/abs/2101.07448), [Men et al. 2021](https://arxiv.org/abs/2108.06152), [Liu et al. 2022](https://arxiv.org/abs/2201.12329)) have argued that one the reasons why DETR requires extremely long training time is that, since in cross-attention each box query attend to all the patches in the image, the network needs to learn how to make each box focus only on a small area of the image by giving high importance only to the patches associated to that area. To make it easier for the network to do this, such works propose different techniques. For example, [Men et al. 2021](https://arxiv.org/abs/2108.06152) propose to concatenate to each patch its positional encoding and to do the same for each query box. Thus, boxes and patches with close spatial positions will have similar positional encodings. In this way, when computing the dot product between them, the similarity will be higher and higher importance will be given to that patch.\n",
        "\n",
        "In our model, we adopt the technique proposed by [Gao et al. 2021](https://arxiv.org/abs/2101.07448). In particular, given a node with associated boundign box coordinates $(c_x, c_y, w, h)$, we compute a Gaussian-like weight map as\n",
        "$$ G(i, j) = \\exp{(- \\frac{(i - c_x)^2}{\\beta w^2} - \\frac{(j - c_y)^2}{\\beta h^2} )} $$\n",
        "where $(i, j) \\in [0, W] \\times [0, H] $ is the position of a patch in the feature map; $\\beta$ is an hyperparameter (here, set to 1) to modulate the width of the Gaussian-like function. When computing the cross-attention matrix between a node and the patches, we sum this weight map to the dot product between the node and the patches. In this way, the network is encouraged to give more importance to the patches close to the center of the node bounding box. For the edges, we compute the Gaussian-like weight map by choosing for each position $(i, j)$ the maximum value between the weight maps of the edge endpoints and the weight map computed using the smallest bounding box containing the two bounding boxes of the endpoints. Thus, similarly to previous works ([Wang et al. 2020](https://arxiv.org/abs/2003.14023)), we assume that the key information to detect a relation is always in the middle of the two bounding boxes, even if it has been shown that this is not always true ([Tamura et al. 2021](https://arxiv.org/abs/2103.05399))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coHdGfYJKpG1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from jaxtyping import Float\n",
        "from torch import Tensor, nn\n",
        "\n",
        "from deepsight.data.structs import BoundingBoxes\n",
        "from deepsight.modeling.layers import LayerScale\n",
        "from deepsight.utils.torch import Batched3DTensors, BatchedGraphs\n",
        "\n",
        "from projects.sgg.modeling import DecoderConfig\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, config: DecoderConfig) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self._num_heads = config.num_heads\n",
        "\n",
        "        self.gaussian_heatmaps = GaussianHeatmaps()\n",
        "        self.node_embeddings = SinusoidalBoxEmbeddings(\n",
        "            config.hidden_dim, include_wh=True\n",
        "        )\n",
        "        self.edge_embeddings = SinusoidalPairwiseBoxEmbeddings(config.hidden_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [DecoderLayer(config) for _ in range(config.num_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        features: Batched3DTensors,\n",
        "        graphs: BatchedGraphs,\n",
        "        boxes: BoundingBoxes,\n",
        "    ) -> list[BatchedGraphs]:\n",
        "        H, W = features.shape[-2:]\n",
        "\n",
        "        # Visual queries matched with the graph nodes, aligned with the edge connections\n",
        "        edge_indices = graphs.edge_indices  # (2, E)\n",
        "        first_boxes = boxes[edge_indices[0]]\n",
        "        second_boxes = boxes[edge_indices[1]]\n",
        "        union_boxes = first_boxes | second_boxes  # (E, 4)\n",
        "\n",
        "        # Masks to account for padding in batch of SceneGraphs (multiple sentences)\n",
        "        node_mask_list = []\n",
        "        edge_mask_list = []\n",
        "        for idx, (num_nodes, num_edges) in enumerate(graphs.sizes):\n",
        "            node_mask_list.append(features.mask[idx, None].expand(num_nodes, -1, -1))\n",
        "            edge_mask_list.append(features.mask[idx, None].expand(num_edges, -1, -1))\n",
        "\n",
        "        nodes_mask = torch.cat(node_mask_list, dim=0)  # (N, H, W)\n",
        "        edges_mask = torch.cat(edge_mask_list, dim=0)  # (E, H, W)\n",
        "\n",
        "        # Build heatmaps for visual queries for cross attention\n",
        "        node_heatmaps = self.gaussian_heatmaps(boxes, nodes_mask)  # (N, H, W)\n",
        "        union_heatmaps = self.gaussian_heatmaps(union_boxes, edges_mask)  # (E, H, W)\n",
        "        first_heatmaps = node_heatmaps[edge_indices[0]]  # (E, H, W)\n",
        "        second_heatmaps = node_heatmaps[edge_indices[1]]  # (E, H, W)\n",
        "        edge_heatmaps = torch.maximum(\n",
        "            torch.maximum(first_heatmaps, second_heatmaps),\n",
        "            union_heatmaps,\n",
        "        )  # (E, H, W)\n",
        "\n",
        "        node_heatmaps = node_heatmaps.flatten(1)  # (N, H * W)\n",
        "        edge_heatmaps = edge_heatmaps.flatten(1)  # (E, H * W)\n",
        "        heatmaps_graph = graphs.new_like(node_heatmaps, edge_heatmaps)\n",
        "\n",
        "        heatmaps = torch.cat(\n",
        "            [\n",
        "                heatmaps_graph.nodes(pad_value=0.0),\n",
        "                heatmaps_graph.edges(pad_value=0.0),\n",
        "            ],\n",
        "            dim=1,\n",
        "        )  # (B, N + E, H * W)\n",
        "\n",
        "        flattened_features = features.to_batched2d()  # (B, H * W, C)\n",
        "        mask = flattened_features.mask[:, None].expand_as(heatmaps)\n",
        "        attn_mask = heatmaps.masked_fill_(mask, -torch.inf)\n",
        "        attn_mask = attn_mask.repeat(self._num_heads, 1, 1)  # (B * heads, N + E, H * W)\n",
        "\n",
        "        # Add bbox sinusoidal embeddings to nodes and edges\n",
        "        node_embeddings = self.node_embeddings(boxes)  # (N, D)\n",
        "        edge_embeddings = self.edge_embeddings(first_boxes, second_boxes)  # (E, D)\n",
        "        embeddings_graph = graphs.new_like(node_embeddings, edge_embeddings)\n",
        "\n",
        "        # graphs: scenegraphs with text embeddings\n",
        "        # embeddings_graph: visual queries with added sinus bbox embeddings\n",
        "        # flattened_features: the visual patch embeddings masked with gaussian heatmaps\n",
        "        layer: DecoderLayer\n",
        "        outputs = []\n",
        "        for layer in self.layers:\n",
        "            graphs = layer(\n",
        "                graphs,\n",
        "                embeddings_graph,\n",
        "                flattened_features.tensor,\n",
        "                attn_mask,\n",
        "            )\n",
        "            outputs.append(graphs)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        features: Batched3DTensors,\n",
        "        graphs: BatchedGraphs,\n",
        "        boxes: BoundingBoxes,\n",
        "    ) -> list[BatchedGraphs]:\n",
        "        return super().__call__(features, graphs, boxes)  # type: ignore\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, config: DecoderConfig) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.pre_cross_attn_layernorm = nn.LayerNorm(config.hidden_dim)\n",
        "        self.cross_attn = nn.MultiheadAttention(\n",
        "            config.hidden_dim,\n",
        "            config.num_heads,\n",
        "            dropout=config.dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.post_cross_attn_layerscale = LayerScale(config.hidden_dim)\n",
        "\n",
        "        self.pre_gat_layernorm = nn.LayerNorm(config.hidden_dim)\n",
        "        self.gat = GATConv(\n",
        "            config.hidden_dim,\n",
        "            config.num_heads,\n",
        "            bias=True,\n",
        "            dropout=config.dropout,\n",
        "        )\n",
        "        self.post_gat_layerscale = LayerScale(config.hidden_dim)\n",
        "\n",
        "        self.pre_ffn_layernorm = nn.LayerNorm(config.hidden_dim)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(config.hidden_dim, 4 * config.hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(config.dropout),\n",
        "            nn.Linear(4 * config.hidden_dim, config.hidden_dim),\n",
        "        )\n",
        "        self.post_ffn_layerscale = LayerScale(config.hidden_dim)\n",
        "\n",
        "    def _perform_cross_attention(\n",
        "        self,\n",
        "        graphs: BatchedGraphs,\n",
        "        features: Float[Tensor, \"B HW D\"],\n",
        "        attn_mask: Float[Tensor, \"Bh (N+E) HW\"],\n",
        "    ) -> BatchedGraphs:\n",
        "        nodes = graphs.nodes(pad_value=0.0)  # (B, N, D)\n",
        "        edges = graphs.edges(pad_value=0.0)  # (B, E, D)\n",
        "        # padded number of nodes and edges\n",
        "        N, E = nodes.shape[1], edges.shape[1]\n",
        "\n",
        "        queries = torch.cat([nodes, edges], dim=1)  # (B, N + E, D)\n",
        "        queries, _ = self.cross_attn(\n",
        "            queries,\n",
        "            features,\n",
        "            features,\n",
        "            attn_mask=attn_mask,\n",
        "            need_weights=False,\n",
        "        )\n",
        "\n",
        "        nodes, edges = torch.split(queries, [N, E], dim=1)\n",
        "        return graphs.new_like(nodes, edges)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        graphs: BatchedGraphs,\n",
        "        embeddings: BatchedGraphs,\n",
        "        features: Float[Tensor, \"B HW D\"],\n",
        "        attn_mask: Float[Tensor, \"Bh (N+E) HW\"],\n",
        "    ) -> BatchedGraphs:\n",
        "        # Perform cross-attention.\n",
        "        nodes, edges = graphs.nodes(None), graphs.edges(None)\n",
        "        N, E = nodes.shape[0], edges.shape[0]\n",
        "        pre_cross_attn_queries = torch.cat([nodes, edges], dim=0)  # (N + E, D)\n",
        "        queries = self.pre_cross_attn_layernorm(pre_cross_attn_queries)\n",
        "        nodes, edges = torch.split(queries, [N, E], dim=0)\n",
        "        graphs = graphs.new_like(nodes, edges)\n",
        "        graphs = self._perform_cross_attention(graphs, features, attn_mask)\n",
        "        nodes, edges = graphs.nodes(None), graphs.edges(None)\n",
        "        queries = torch.cat([nodes, edges], dim=0)  # (N + E, D)\n",
        "        post_cross_attn_queries = (\n",
        "            pre_cross_attn_queries + self.post_cross_attn_layerscale(queries)\n",
        "        )\n",
        "\n",
        "        # Perform GAT\n",
        "        pre_gat_queries = post_cross_attn_queries\n",
        "        pre_gat_queries = self.pre_gat_layernorm(pre_gat_queries)\n",
        "        nodes, edges = torch.split(pre_gat_queries, [N, E], dim=0)\n",
        "        graphs = graphs.new_like(nodes, edges)\n",
        "        graphs = self.gat(graphs, embeddings)\n",
        "        nodes, edges = graphs.nodes(None), graphs.edges(None)\n",
        "        queries = torch.cat([nodes, edges], dim=0)  # (N + E, D)\n",
        "        post_gat_queries = pre_gat_queries + self.post_gat_layerscale(queries)\n",
        "\n",
        "        # Perform FFN\n",
        "        pre_ffn_queries = post_gat_queries\n",
        "        queries = self.pre_ffn_layernorm(pre_gat_queries)\n",
        "        queries = self.ffn(queries)\n",
        "        post_fnn_queries = pre_ffn_queries + self.post_ffn_layerscale(queries)\n",
        "\n",
        "        # Update graphs\n",
        "        nodes, edges = torch.split(post_fnn_queries, [N, E], dim=0)\n",
        "        return graphs.new_like(nodes, edges)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        graphs: BatchedGraphs,\n",
        "        embeddings: BatchedGraphs,\n",
        "        features: Float[Tensor, \"B HW D\"],\n",
        "        attn_mask: Float[Tensor, \"Bh (N+E) HW\"],\n",
        "    ) -> BatchedGraphs:\n",
        "        return super().__call__(graphs, embeddings, features, attn_mask)  # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87fMBjkxKpG1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from jaxtyping import Integer\n",
        "from torch import Tensor, nn\n",
        "\n",
        "from deepsight.data.structs import (\n",
        "    Batch,\n",
        "    BoundingBoxes,\n",
        "    BoundingBoxFormat,\n",
        "    ODInput,\n",
        "    ODOutput,\n",
        "    SceneGraph,\n",
        ")\n",
        "from deepsight.modeling.detectors import OwlViT\n",
        "from deepsight.modeling.layers import clip\n",
        "from deepsight.modeling.pipeline import Model as _Model\n",
        "from deepsight.utils.torch import BatchedGraphs, Graph\n",
        "\n",
        "from projects.sgg.modeling import Config\n",
        "from projects.sgg.modeling._structs import ModelInput, ModelOutput, TextEmbeddings\n",
        "\n",
        "\n",
        "class Model(_Model[ModelInput, ModelOutput]):\n",
        "    def __init__(self, config: Config) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.vision_encoder = VisionEncoder(\n",
        "            config.encoders.model,\n",
        "            config.encoders.output_dim,\n",
        "        )\n",
        "\n",
        "        self.text_encoder = TextEncoder(\n",
        "            config.encoders.model,\n",
        "            config.encoders.output_dim,\n",
        "        )\n",
        "\n",
        "        self.detector = OwlViT(\n",
        "            config.detector.box_threshold, config.detector.num_queries\n",
        "        )\n",
        "\n",
        "        self.same_entity_edge = nn.Parameter(torch.randn(1, config.encoders.output_dim))\n",
        "        self.decoder = Decoder(config.decoder)\n",
        "\n",
        "        # before computing the similarity between a node and the caption embedding, we project the node\n",
        "        # indeed the caption embedding itself is obtained by projecting the pooled output of the CLIP text transformer\n",
        "        # since both projections are learned, we can assume that the similarity is computed in the same space\n",
        "        self.projection = nn.Linear(\n",
        "            config.decoder.hidden_dim, config.decoder.hidden_dim\n",
        "        )\n",
        "\n",
        "        self.regression_head = nn.Sequential(\n",
        "            nn.Linear(config.decoder.hidden_dim, config.decoder.hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(config.decoder.dropout),\n",
        "            nn.Linear(config.decoder.hidden_dim, 4),\n",
        "        )\n",
        "\n",
        "    def _get_text_embeddings(self, inputs: ModelInput) -> list[TextEmbeddings]:\n",
        "        texts = []\n",
        "        for caption, graph in zip(inputs.captions, inputs.graphs):\n",
        "            # here we do not add the article 'a' after 'a photo of' since\n",
        "            # most entities already have the article in their phrase\n",
        "            texts.append(caption)\n",
        "            texts.extend(f\"a photo of {e.phrase}\" for e in graph.entities())\n",
        "            texts.extend(\n",
        "                f\"a photo of {r.subject.phrase} {r.relation} {r.object.phrase}\"\n",
        "                for r in graph.triplets(None, False, False)\n",
        "            )\n",
        "\n",
        "        tmp = self.text_encoder(texts)\n",
        "\n",
        "        embeddings = []\n",
        "        count = 0\n",
        "\n",
        "        for graph in inputs.graphs:\n",
        "            caption_emb = tmp[count]\n",
        "            count += 1\n",
        "\n",
        "            entities_emb = tmp[count : count + len(graph.entities())]\n",
        "            count += len(graph.entities())\n",
        "\n",
        "            num_relations = len(graph.triplets(None, True, True))\n",
        "            relations_emb = tmp[count : count + num_relations]\n",
        "            count += num_relations\n",
        "\n",
        "            embeddings.append(\n",
        "                TextEmbeddings(\n",
        "                    entities=entities_emb,\n",
        "                    relations=relations_emb,\n",
        "                    caption=caption_emb,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def _get_detections(self, inputs: ModelInput) -> list[ODOutput]:\n",
        "        batch = Batch(\n",
        "            [\n",
        "                ODInput(image, [e.noun for e in graph.entities()])\n",
        "                for image, graph in zip(inputs.images, inputs.graphs)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        return list(self.detector(batch))\n",
        "\n",
        "    def _get_graph(\n",
        "        self,\n",
        "        graph: SceneGraph,\n",
        "        embeddings: TextEmbeddings,\n",
        "        detections: ODOutput,\n",
        "    ) -> Graph:\n",
        "        device = embeddings.entities.device\n",
        "        num_relations = embeddings.relations.shape[0]\n",
        "\n",
        "        edge_index_list: list[Integer[Tensor, \"2 N\"]] = []\n",
        "        rel_index_list: list[int] = []\n",
        "\n",
        "        for det_idx, detection in enumerate(detections.entities):\n",
        "            entity_idx = int(detection)\n",
        "\n",
        "            # add relations between entities based on the scene graph\n",
        "            for rel in graph.triplets(entity_idx, True, True):\n",
        "                end = (detections.entities == rel.object).nonzero(as_tuple=True)[0]\n",
        "                end = end[None]  # (1, K)\n",
        "\n",
        "                start = torch.tensor([det_idx], device=device).expand_as(end)\n",
        "                indexes = torch.cat([start, end], dim=0)  # (2, K)\n",
        "\n",
        "                edge_index_list.append(indexes)\n",
        "                rel_index_list.extend([rel.relation] * indexes.shape[1])\n",
        "\n",
        "            # add relations between instances of the same entity\n",
        "            end = (detections.entities == entity_idx).nonzero(as_tuple=True)[0]\n",
        "            end = end[None]  # (1, K)\n",
        "\n",
        "            start = torch.tensor([det_idx], device=device).expand_as(end)\n",
        "            indexes = torch.cat([start, end], dim=0)  # (2, K)\n",
        "\n",
        "            edge_index_list.append(indexes)\n",
        "            rel_index_list.extend([num_relations] * indexes.shape[1])\n",
        "\n",
        "        edge_indices = torch.cat(edge_index_list, dim=1)  # (2, E)\n",
        "\n",
        "        relations_emb = torch.cat([embeddings.relations, self.same_entity_edge])\n",
        "        rel_indices = torch.tensor(rel_index_list, device=device)\n",
        "        relations = relations_emb[rel_indices]  # (E, D)\n",
        "\n",
        "        nodes = embeddings.entities[detections.entities]  # (N, D)\n",
        "\n",
        "        return Graph(\n",
        "            nodes=nodes,\n",
        "            edges=relations,\n",
        "            edge_indices=edge_indices,\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs: ModelInput) -> ModelOutput:\n",
        "        features = self.vision_encoder(inputs.features)\n",
        "        embeddings = self._get_text_embeddings(inputs)\n",
        "        detections = self._get_detections(inputs)\n",
        "\n",
        "        tmp = [\n",
        "            self._get_graph(graph, embedding, detection)\n",
        "            for graph, embedding, detection in zip(\n",
        "                inputs.graphs, embeddings, detections\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Build decoder inputs\n",
        "        graph = BatchedGraphs.from_list(tmp)\n",
        "        boxes = BoundingBoxes.cat([detection.boxes for detection in detections])\n",
        "        graphs = self.decoder(features, graph, boxes)\n",
        "\n",
        "        # Compute new boxes\n",
        "        base_boxes = BoundingBoxes.pad_sequence(\n",
        "            [detection.boxes for detection in detections]\n",
        "        )  # (B, N, 4)\n",
        "        base_boxes = base_boxes.to_cxcywh().normalize()\n",
        "\n",
        "        new_boxes = []\n",
        "        for idx in range(len(graphs)):\n",
        "            graph = graphs[idx]\n",
        "\n",
        "            nodes = graph.nodes(pad_value=0)  # (B, N, D)\n",
        "            offsets = self.regression_head(nodes)  # (B, N, 4)\n",
        "            box_tensor = torch.logit(base_boxes.tensor) + offsets\n",
        "            box_tensor = torch.sigmoid(box_tensor)\n",
        "            box = BoundingBoxes(\n",
        "                box_tensor,\n",
        "                base_boxes.images_size,\n",
        "                format=BoundingBoxFormat.CXCYWH,\n",
        "                normalized=True,\n",
        "            )\n",
        "            new_boxes.append(box)\n",
        "\n",
        "            nodes = graph.nodes(None)  # (N, D)\n",
        "            nodes = self.projection(nodes)  # (N, D)\n",
        "            graphs[idx] = graph.new_like(nodes=nodes, clone=False)\n",
        "\n",
        "        max_detections = max(len(detection.entities) for detection in detections)\n",
        "        padded_entities = torch.nn.utils.rnn.pad_sequence(\n",
        "            [detection.entities for detection in detections],\n",
        "            batch_first=True,\n",
        "            padding_value=max_detections,\n",
        "        )\n",
        "\n",
        "        return ModelOutput(\n",
        "            captions=torch.stack([embedding.caption for embedding in embeddings]),\n",
        "            graphs=graphs,\n",
        "            boxes=new_boxes,\n",
        "            padded_entities=padded_entities,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdWtuwKNKpG2"
      },
      "source": [
        "##### Criterion\n",
        "\n",
        "The criterion is responsible for the loss calculation. To calculate the loss we first need to choose which of the candidate bounding boxes to associate to the ground truth bounding box. As previously said, we only consider the bounding boxes obtained from the graph nodes that correspond to the entity that is the subject of the region description, sine this is the target entity to detect. Notice that here we make the assumption that the first entity of the scene graph is the subject of the region description. We deemed not necessary to apply a NLP tool to extract the subject of the sentence since we never observed a case in the dataset where the subject was not the first entity.\n",
        "\n",
        "To decide which of the candidate bounding boxes match to the target bounding box, we adopt the same approach used by DETR-like models. In particular, for each candidate bounding box, we compute the cost of matching it with the ground truth. The cost is computed as the weighted sum of three differents factors:\n",
        "1. the L1 distance between the coordinates of the candidate bounding box and the ground truth bounding box;\n",
        "2. the Generalized IoU between the candidate bounding box and the ground truth bounding box;\n",
        "3. the negative cosine similarity between the node embedding of the candidate bounding box and the text embedding of the region description.\n",
        "Notice that since there is only one ground truth bounding box, we do not need to apply the full Hungarian matching algorithm, but we can simple select the candidate that minimizes the cost.\n",
        "\n",
        "Once the matching is done, we can compute the loss between the matched bounding boxes. The loss is computed as the weighted sum of three different losses:\n",
        "1. the L1 loss between the coordinates of the matched bounding boxes;\n",
        "2. the Generalized IoU ([Rezatofighi et al. 2019](https://arxiv.org/abs/1902.09630)) loss between the matched bounding boxes;\n",
        "3. the InfoNCE loss ([Oord et al. 2018](https://arxiv.org/abs/1807.03748)) where the positive sample is the embedding of the matched node and the negative samples are the embeddings of the other nodes.\n",
        "\n",
        "The first two losses are the canonical losses used by DETR-like models to make the network learn to correctly predict the bounding box coordinates. Here, we remove the classification loss (usually computed using focal loss) since we do not need to predict the class of the selected node; its class is already known from the input scene graph. However, since at inference time we select among the candidate nodes the one with the highest similarity with the text embedding of the region description, we use a contrastive loss (here, InfoNCE) to force the network to pull together the embeddings of the matched node and the text embedding of the region description and to push away the embeddings of the other wrong nodes. Since it has been shown that InfoNCE performs better when the number of negative samples is high, as negative samples we do not use only the other subject nodes but all the nodes of all the graphs in the same batch.\n",
        "\n",
        "Finally, similarly to other DETR-like models, the loss is computed not only with respect to the output of the last decoder layer but also with respect to the output of the intermediate decoder layers. In particular, for each layer we recompute the matching and the corresponding loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6j4YzG9KpG3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from deepsight.data.structs import Batch, BoundingBoxes, RECOutput\n",
        "from deepsight.measures import Loss, Reduction\n",
        "from deepsight.measures.losses import BoxL1Loss, GeneralizedBoxIoULoss, InfoNCELoss\n",
        "from deepsight.modeling.pipeline import Criterion as _Criterion\n",
        "\n",
        "from projects.sgg.modeling._config import CriterionConfig\n",
        "from projects.sgg.modeling._structs import ModelOutput\n",
        "\n",
        "\n",
        "class Criterion(_Criterion[ModelOutput, RECOutput]):\n",
        "    def __init__(self, config: CriterionConfig) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.auxiliary = config.auxiliary\n",
        "        self.num_layers = config.num_layers\n",
        "\n",
        "        self.l1_cost = config.l1_cost\n",
        "        self.giou_cost = config.giou_cost\n",
        "        self.similarity_cost = config.similarity_cost\n",
        "\n",
        "        self.l1_weight = config.l1_weight\n",
        "        self.giou_weight = config.giou_weight\n",
        "        self.infonce_weight = config.infonce_weight\n",
        "\n",
        "        self.l1_loss = BoxL1Loss(reduction=Reduction.NONE)\n",
        "        self.giou_loss = GeneralizedBoxIoULoss(reduction=Reduction.NONE)\n",
        "        self.infonce_loss = InfoNCELoss(\n",
        "            temperature=config.temperature, reduction=Reduction.MEAN\n",
        "        )\n",
        "\n",
        "    def losses_names(self) -> list[str]:\n",
        "        losses = []\n",
        "        if self.auxiliary:\n",
        "            losses += [f\"L1_{i}\" for i in range(self.num_layers)]\n",
        "            losses += [f\"GIoU_{i}\" for i in range(self.num_layers)]\n",
        "            losses += [f\"InfoNCE_{i}\" for i in range(self.num_layers)]\n",
        "        else:\n",
        "            losses += [\"L1\", \"GIoU\", \"InfoNCE\"]\n",
        "\n",
        "        return losses\n",
        "\n",
        "    def _compute_layer_loss(\n",
        "        self,\n",
        "        output: ModelOutput,\n",
        "        tgt_boxes: BoundingBoxes,\n",
        "        layer_idx: int,\n",
        "    ) -> list[Loss]:\n",
        "        \"\"\"Computes the loss for the output of a single layer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        output : ModelOutput\n",
        "            The output of the model.\n",
        "        tgt_boxes : BoundingBoxes\n",
        "            The target boxes. The tensor has shape (B, N, 4).\n",
        "        layer_idx : int\n",
        "            The index of the layer.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        list[Loss]\n",
        "            A list of the computed losses.\n",
        "        \"\"\"\n",
        "\n",
        "        B, N = output.padded_entities.shape\n",
        "        subject_mask = output.padded_entities != 0\n",
        "        padding_mask = output.padded_entities == N\n",
        "\n",
        "        out_boxes = output.boxes[layer_idx].to_cxcywh().normalize()  # (B, N, 4)\n",
        "\n",
        "        l1_loss = self.l1_loss(out_boxes, tgt_boxes)  # (B, N)\n",
        "        giou_loss = self.giou_loss(out_boxes, tgt_boxes)  # (B, N)\n",
        "\n",
        "        nodes = output.graphs[layer_idx].nodes(pad_value=0.0)  # (B, N, D)\n",
        "        captions = output.captions.unsqueeze(1).expand(-1, N, -1)  # (B, N, D)\n",
        "        similarity = torch.cosine_similarity(nodes, captions, dim=-1)  # (B, N)\n",
        "\n",
        "        cost = (\n",
        "            self.l1_cost * l1_loss\n",
        "            + self.giou_cost * giou_loss\n",
        "            - self.similarity_cost * similarity\n",
        "        )\n",
        "\n",
        "        cost = cost.masked_fill_(subject_mask, torch.inf)  # (B, N)\n",
        "        idx = cost.min(dim=1)[1]  # (B,)\n",
        "\n",
        "        pos_mask = torch.zeros_like(output.padded_entities, dtype=torch.bool)  # (B, N)\n",
        "        pos_mask[torch.arange(B), idx] = True\n",
        "\n",
        "        nodes = output.graphs[layer_idx].nodes(pad_value=0.0)  # (B, N, D)\n",
        "        queries = output.captions  # (B, D)\n",
        "        pos_keys = nodes[pos_mask]\n",
        "        neg_mask = torch.logical_xor(pos_mask, ~padding_mask)\n",
        "        neg_keys = nodes[neg_mask]\n",
        "        infonce_loss = self.infonce_loss(queries, pos_keys, neg_keys)  # (B,)\n",
        "\n",
        "        l1_loss = l1_loss[pos_mask].mean()\n",
        "        giou_loss = giou_loss[pos_mask].mean()\n",
        "\n",
        "        if layer_idx == -1:\n",
        "            return [\n",
        "                Loss(\"L1\", l1_loss, self.l1_weight),\n",
        "                Loss(\"GIoU\", giou_loss, self.giou_weight),\n",
        "                Loss(\"InfoNCE\", infonce_loss, self.infonce_weight),\n",
        "            ]\n",
        "        else:\n",
        "            return [\n",
        "                Loss(f\"L1_{layer_idx}\", l1_loss, self.l1_weight),\n",
        "                Loss(f\"GIoU_{layer_idx}\", giou_loss, self.giou_weight),\n",
        "                Loss(f\"InfoNCE_{layer_idx}\", infonce_loss, self.infonce_weight),\n",
        "            ]\n",
        "\n",
        "    def forward(self, output: ModelOutput, targets: Batch[RECOutput]) -> list[Loss]:\n",
        "        B, N = output.padded_entities.shape\n",
        "\n",
        "        tgt_boxes = BoundingBoxes.stack([tgt.box for tgt in targets], dim=0)  # (B, 4)\n",
        "        tgt_boxes = tgt_boxes.to_cxcywh().normalize()  # (B, 4)\n",
        "        tgt_boxes = tgt_boxes.unsqueeze(1).expand(-1, N, -1)  # (B, N, 4)\n",
        "\n",
        "        if self.auxiliary:\n",
        "            losses = []\n",
        "            for i in range(self.num_layers):\n",
        "                losses += self._compute_layer_loss(output, tgt_boxes, i)\n",
        "        else:\n",
        "            losses = self._compute_layer_loss(output, tgt_boxes, -1)\n",
        "\n",
        "        return losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez78MJkkKpG3"
      },
      "source": [
        "##### PostProcessor\n",
        "\n",
        "As previously described, to obtain the candidate bounding box, from the graph outputted by the last decoder layer, we select the nodes that refer to the subject of the region description. Then, the similarity between the node embeddings and the text embedding of the description is computed using cosine similarity. The bounding box associated to the nodes with the highest similarity is then returned as the candidate bounding box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwzntWzgKpG3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from deepsight.data.structs import Batch, RECOutput\n",
        "from deepsight.modeling.pipeline import PostProcessor as _PostProcessor\n",
        "\n",
        "from projects.sgg.modeling._structs import ModelOutput\n",
        "\n",
        "\n",
        "class PostProcessor(_PostProcessor[ModelOutput, RECOutput]):\n",
        "    def forward(self, output: ModelOutput) -> Batch[RECOutput]:\n",
        "        B, N = output.padded_entities.shape\n",
        "        subject_mask = output.padded_entities != 0  # (B, N)\n",
        "\n",
        "        queries = output.captions.unsqueeze(1)  # (B, 1, D)\n",
        "        keys = output.graphs[-1].nodes(pad_value=0.0)  # (B, N, D)\n",
        "\n",
        "        similarity = torch.cosine_similarity(queries, keys, dim=-1)  # (B, N)\n",
        "        similarity.masked_fill_(subject_mask, -torch.inf)  # (B, N)\n",
        "\n",
        "        idx = similarity.max(dim=1)[1]  # (B,)\n",
        "\n",
        "        boxes = output.boxes[-1][torch.arange(B), idx]  # (B, 4)\n",
        "\n",
        "        return Batch([RECOutput(box=boxes[i]) for i in range(B)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-gyXZosKpG4"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XhLN2MQKpG4"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-1qB4N6KpG4"
      },
      "source": [
        "#### Implementation details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G7XnEPeKpG4"
      },
      "source": [
        "The hyperparameters chosen to train the network are mostly based on the ones used by the detection papers our work is based on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MThxQcadKpG5"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from deepsight.modeling.layers.clip import Models\n",
        "from projects.sgg.modeling import (\n",
        "    Config,\n",
        "    CriterionConfig,\n",
        "    DecoderConfig,\n",
        "    DetectorConfig,\n",
        "    EncodersConfig,\n",
        "    PreprocessorConfig,\n",
        ")\n",
        "\n",
        "pipeline_config = Config(\n",
        "    preprocessor=PreprocessorConfig(\n",
        "        file=Path(\"./data/refcocog/annotations/scene_graphs.json\"),\n",
        "        token=\"\",\n",
        "        # the same resolution is used by Grounding DINO\n",
        "        side=800,\n",
        "        max_side=1333,\n",
        "        # mean and std of the CLIP model\n",
        "        mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "        std=[0.26862954, 0.26130258, 0.27577711],\n",
        "    ),\n",
        "    encoders=EncodersConfig(\n",
        "        output_dim=256,\n",
        "        # we choose the smallest ViT model to reduce training times and memory usage\n",
        "        model=Models.ViT_B_32_224,\n",
        "    ),\n",
        "    detector=DetectorConfig(\n",
        "        # we found that a threshold of 0.25 is the smallest value that allows the model to detect most of the entities\n",
        "        # without retuning bounding boxes for instances that are clearly not present in the image\n",
        "        box_threshold=0.25,\n",
        "        # the number of bounding box to return for an entity that was not found with the box_threshold\n",
        "        # we set this number to 4 such that the returned boundig box can uniformly cover the whole image\n",
        "        num_queries=4,\n",
        "    ),\n",
        "    decoder=DecoderConfig(\n",
        "        # 256 is the typical hidden dimension used by all object detection models inspired by DETR\n",
        "        hidden_dim=256,\n",
        "        # In DETR-like models, the decoder usually consists of 6 layers. Here, to reduce training times and memory usage,\n",
        "        # we use only 3 layers. However, it has been shown that the performance of such models is not extremely sensitive\n",
        "        # to the number of layers used in the decoder. Notice that the number of decoder layers also limits the nodes\n",
        "        # with which a graph node can exchange information (this is a problem of traditional MPNNs).\n",
        "        # In this case, each node can exhange directly or indirectly information with all the nodes at a maximum distance of 3.\n",
        "        # This is not a problem since there are no scene graphs with a depth greater than 3.\n",
        "        num_layers=3,\n",
        "        num_heads=8,\n",
        "        # Since our decoder has less than 18 layers, the initial epsilon value is set to 0.1\n",
        "        # (see https://paperswithcode.com/method/layerscale)\n",
        "        epsilon_layer_scale=0.1,\n",
        "    ),\n",
        "    criterion=CriterionConfig(\n",
        "        # we use the same cost and loss weights employed by DINO\n",
        "        # in this case, since we substitute the classification loss with the InfoNCE loss,\n",
        "        # we use the weights of the classification loss for the InfoNCE loss\n",
        "        # indeed, in our case the InfoNCE can be seen as a sort of classification loss,\n",
        "        # since it pulls together the embeddings of the query and the correct subject node\n",
        "        # and the similatity of these embeddings is used to \"classify\" the right node and the wrong nodes\n",
        "        l1_cost=5.0,\n",
        "        giou_cost=2.0,\n",
        "        similarity_cost=2.0,\n",
        "        l1_weight=5.0,\n",
        "        giou_weight=2.0,\n",
        "        infonce_weight=1.0,\n",
        "        # whether to compute the loss also for intermediate layers\n",
        "        # DETR-like models usually compute the loss for all layers\n",
        "        auxiliary=True,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ted_z6e0KpG5"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from datasets.refcocog import Config as RefCOCOGConfig\n",
        "from deepsight.engines.trainer import Config, Params\n",
        "from deepsight.lr_schedulers.torch import Config as LRSchedulerConfig\n",
        "from deepsight.optimizers.torch import Config as OptimizerConfig\n",
        "from deepsight.optimizers.torch import ParamGroupConfig\n",
        "from deepsight.utils import wandb\n",
        "from deepsight.utils.torch import FloatType\n",
        "\n",
        "train_config = Config(\n",
        "    dir=Path(\"./output/sgg\"),\n",
        "    wandb=wandb.Config(\n",
        "        job_type=\"train\",\n",
        "        enabled=False,\n",
        "        project=\"sgg\",\n",
        "        entity=\"visgator\",\n",
        "    ),\n",
        "    debug=False,\n",
        "    params=Params(\n",
        "        num_epochs=24,\n",
        "        # the total batch size is set to 256, however we implement gradient accumulation\n",
        "        # since 256 samples do not fit in a single gpu H100\n",
        "        train_batch_size=256,\n",
        "        eval_batch_size=16,\n",
        "        gradient_accumulation_steps=16,\n",
        "        max_grad_norm=5.0,\n",
        "        # we use a smaller init_scale that the default one (2**16) since the default ones lead to overflows\n",
        "        init_scale=2**12,\n",
        "        seed=3407, # https://arxiv.org/abs/2109.08203\n",
        "        # we use mixed precision training to reduce memory usage and training times\n",
        "        dtype=FloatType.FLOAT16,\n",
        "        dataset=RefCOCOGConfig(path=Path(\"./data/refcocog\")),\n",
        "        pipeline=pipeline_config,\n",
        "        # We use AdamW as optimizer as recommended by FastAI and it is the same optimizer used by DINO and Grounding DINO.\n",
        "        # Since CLIP is trained on an image-text association task, while we want the vision encoder to encode local information\n",
        "        # in each patch, we have decided to finetune the CLIP encoders. Since the same reasoning is at the base of OwlViT, we\n",
        "        # use the same learning rate and weight decay values used by OwlViT to finetune the imaget-text constrative pretrained text and vision encoder. 7\n",
        "        # In particular, they show that it is fundamental to use a much smaller learning rate (100x smaller) for the text encoder\n",
        "        # to reduce overfitting, possibly by preventing the text encoder from “forgetting” the semantics learned during pre-training\n",
        "        # while fine-tuning on the small space of detection label.\n",
        "        # The weight decay value for the decoder is the same used by DINO. With respect to DINO that uses a learning rate of 1e-4,\n",
        "        # we increase it to 5e-4 since we use OneCycleLR and thus the learning rate is much smaller than the maximum value for most of the training.\n",
        "        # Since the projections of the vision and text encoder must be trained from scratch, we use the same learning rate and weight decay\n",
        "        # of the decoder.\n",
        "        optimizer=OptimizerConfig(\n",
        "            \"AdamW\",\n",
        "            groups=[\n",
        "                ParamGroupConfig(\n",
        "                    regex=r\"model.vision_encoder.projection.*\",\n",
        "                    args={\"lr\": 5e-4, \"weight_decay\": 1e-4},\n",
        "                ),\n",
        "                ParamGroupConfig(\n",
        "                    regex=r\"model.text_encoder.projection.*\",\n",
        "                    args={\"lr\": 5e-4, \"weight_decay\": 1e-4},\n",
        "                ),\n",
        "                ParamGroupConfig(\n",
        "                    regex=r\"model.vision_encoder.*\",\n",
        "                    args={\"lr\": 2e-4, \"weight_decay\": 0.0},\n",
        "                ),\n",
        "                ParamGroupConfig(\n",
        "                    regex=r\"model.text_encoder.*\",\n",
        "                    args={\"lr\": 2e-6, \"weight_decay\": 0.0},\n",
        "                ),\n",
        "                ParamGroupConfig(\n",
        "                    regex=r\"model.*\", args={\"lr\": 5e-4, \"weight_decay\": 1e-4}\n",
        "                ),\n",
        "            ],\n",
        "            # we do not finetune the detector since it is pretrained on much more data than RefCOCOg\n",
        "            # thus by finetuning the detector may lose its detection capabilities on a variety of scenes\n",
        "            # not present in the training set of RefCOCOg\n",
        "            freeze=[\"model.detector.*\"],\n",
        "        ),\n",
        "        # as learning rate scheduler we use the OneCycleLR scheduler since it is the one recommended by FastAI\n",
        "        # we also use the same default behaviour of the FastAI implementation\n",
        "        lr_scheduler=LRSchedulerConfig(\n",
        "            \"OneCycleLR\", args={\"max_lr\": [5e-4, 5e-4, 2e-4, 2e-6, 5e-4]}\n",
        "        ),\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do not report here the code of the trainer since it would only clutter the notebook and the its implementation follows the standard PyTorch training logic. The code can be found at `deepsight.engines.trainer._trainer`."
      ],
      "metadata": {
        "id": "Yjsicjn6q2bG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/visgator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lfTIXsqrNOK",
        "outputId": "5f4f798a-20fe-46fb-db11-4dae9d1b55e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/visgator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To start the training, run:\n",
        "\n",
        "from typing import Any\n",
        "\n",
        "from deepsight.engines.trainer import Trainer\n",
        "\n",
        "# note the training will fail since the batch size is too high for colab\n",
        "# we do not modify the hyperparameters since they are the ones used in the training run\n",
        "# eventually, it is sufficient to set an higher number as gradient_accumulation_steps\n",
        "trainer: Trainer[Any, Any] = Trainer.new(train_config)\n",
        "trainer.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QbpxTvipqzNG",
        "outputId": "beaeb5bc-1f32-4d0f-f045-eec5ada2d1f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2023-07-11 20:47:21] INFO: Using device cuda:0.\n",
            "[2023-07-11 20:47:21] INFO: Using device cuda:0.\n",
            "[2023-07-11 20:47:21] INFO: Using device cuda:0.\n",
            "[2023-07-11 20:47:21] INFO: Using device cuda:0.\n",
            "[2023-07-11 20:47:21] INFO: Using device cuda:0.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "[2023-07-11 20:47:54] INFO: Using RefCOCOg dataset.\n",
            "[2023-07-11 20:47:54] INFO: Using RefCOCOg dataset.\n",
            "[2023-07-11 20:47:54] INFO: Using RefCOCOg dataset.\n",
            "[2023-07-11 20:47:54] INFO: Using RefCOCOg dataset.\n",
            "[2023-07-11 20:47:54] INFO: Using RefCOCOg dataset.\n",
            "[2023-07-11 20:47:54] INFO: \t(train) size: 80506 | (eval) size: 4896\n",
            "[2023-07-11 20:47:54] INFO: \t(train) size: 80506 | (eval) size: 4896\n",
            "[2023-07-11 20:47:54] INFO: \t(train) size: 80506 | (eval) size: 4896\n",
            "[2023-07-11 20:47:54] INFO: \t(train) size: 80506 | (eval) size: 4896\n",
            "[2023-07-11 20:47:54] INFO: \t(train) size: 80506 | (eval) size: 4896\n",
            "[2023-07-11 20:47:54] INFO: \t(train) batch size: 256 | (eval) batch size: 16\n",
            "[2023-07-11 20:47:54] INFO: \t(train) batch size: 256 | (eval) batch size: 16\n",
            "[2023-07-11 20:47:54] INFO: \t(train) batch size: 256 | (eval) batch size: 16\n",
            "[2023-07-11 20:47:54] INFO: \t(train) batch size: 256 | (eval) batch size: 16\n",
            "[2023-07-11 20:47:54] INFO: \t(train) batch size: 256 | (eval) batch size: 16\n",
            "[2023-07-11 20:47:54] INFO: \t(train) gradient accumulation steps: 16\n",
            "[2023-07-11 20:47:54] INFO: \t(train) gradient accumulation steps: 16\n",
            "[2023-07-11 20:47:54] INFO: \t(train) gradient accumulation steps: 16\n",
            "[2023-07-11 20:47:54] INFO: \t(train) gradient accumulation steps: 16\n",
            "[2023-07-11 20:47:54] INFO: \t(train) gradient accumulation steps: 16\n",
            "Some weights of the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing CLIPVisionModelWithProjection: ['text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'logit_scale', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.final_layer_norm.weight', 'text_projection.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight']\n",
            "- This IS expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing CLIPTextModelWithProjection: ['vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'logit_scale', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'visual_projection.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight']\n",
            "- This IS expected if you are initializing CLIPTextModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2023-07-11 20:48:19] INFO: Using Scene Graph Grounder pipeline.\n",
            "[2023-07-11 20:48:19] INFO: Using Scene Graph Grounder pipeline.\n",
            "[2023-07-11 20:48:19] INFO: Using Scene Graph Grounder pipeline.\n",
            "[2023-07-11 20:48:19] INFO: Using Scene Graph Grounder pipeline.\n",
            "[2023-07-11 20:48:19] INFO: Using Scene Graph Grounder pipeline.\n",
            "[2023-07-11 20:48:19] INFO: Using AdamW optimizer.\n",
            "[2023-07-11 20:48:19] INFO: Using AdamW optimizer.\n",
            "[2023-07-11 20:48:19] INFO: Using AdamW optimizer.\n",
            "[2023-07-11 20:48:19] INFO: Using AdamW optimizer.\n",
            "[2023-07-11 20:48:19] INFO: Using AdamW optimizer.\n",
            "[2023-07-11 20:48:19] INFO: Using OneCycleLR lr scheduler.\n",
            "[2023-07-11 20:48:19] INFO: Using OneCycleLR lr scheduler.\n",
            "[2023-07-11 20:48:19] INFO: Using OneCycleLR lr scheduler.\n",
            "[2023-07-11 20:48:19] INFO: Using OneCycleLR lr scheduler.\n",
            "[2023-07-11 20:48:19] INFO: Using OneCycleLR lr scheduler.\n",
            "[2023-07-11 20:48:20] INFO: Training started.\n",
            "[2023-07-11 20:48:20] INFO: Training started.\n",
            "[2023-07-11 20:48:20] INFO: Training started.\n",
            "[2023-07-11 20:48:20] INFO: Training started.\n",
            "[2023-07-11 20:48:20] INFO: Training started.\n",
            "[2023-07-11 20:48:20] INFO: Epoch 1/24 started.\n",
            "[2023-07-11 20:48:20] INFO: Epoch 1/24 started.\n",
            "[2023-07-11 20:48:20] INFO: Epoch 1/24 started.\n",
            "[2023-07-11 20:48:20] INFO: Epoch 1/24 started.\n",
            "[2023-07-11 20:48:20] INFO: Epoch 1/24 started.\n",
            "[2023-07-11 20:48:20] INFO: Training epoch 1 started.\n",
            "[2023-07-11 20:48:20] INFO: Training epoch 1 started.\n",
            "[2023-07-11 20:48:20] INFO: Training epoch 1 started.\n",
            "[2023-07-11 20:48:20] INFO: Training epoch 1 started.\n",
            "[2023-07-11 20:48:20] INFO: Training epoch 1 started.\n",
            "Training:   0%|          | 0/80384 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Training:   0%|          | 0/80384 [00:03<?, ?it/s]\n",
            "[2023-07-11 20:48:23] ERROR: Training failed with the following error: CUDA out of memory. Tried to allocate 1.31 GiB (GPU 0; 14.75 GiB total capacity; 12.00 GiB already allocated; 22.81 MiB free; 13.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "[2023-07-11 20:48:23] ERROR: Training failed with the following error: CUDA out of memory. Tried to allocate 1.31 GiB (GPU 0; 14.75 GiB total capacity; 12.00 GiB already allocated; 22.81 MiB free; 13.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "[2023-07-11 20:48:23] ERROR: Training failed with the following error: CUDA out of memory. Tried to allocate 1.31 GiB (GPU 0; 14.75 GiB total capacity; 12.00 GiB already allocated; 22.81 MiB free; 13.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "[2023-07-11 20:48:23] ERROR: Training failed with the following error: CUDA out of memory. Tried to allocate 1.31 GiB (GPU 0; 14.75 GiB total capacity; 12.00 GiB already allocated; 22.81 MiB free; 13.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "[2023-07-11 20:48:23] ERROR: Training failed with the following error: CUDA out of memory. Tried to allocate 1.31 GiB (GPU 0; 14.75 GiB total capacity; 12.00 GiB already allocated; 22.81 MiB free; 13.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-e308eae00cbd>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# eventually, it is sufficient to set an higher number as gradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/visgator/deepsight/engines/trainer/_trainer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/visgator/deepsight/engines/trainer/_trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, start_epoch)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1}/{params.num_epochs} started.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/visgator/deepsight/engines/trainer/_trainer.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    460\u001b[0m                 ):\n\u001b[1;32m    461\u001b[0m                     \u001b[0mmodel_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m                     \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m                     \u001b[0mcrt_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/visgator/deepsight/modeling/pipeline/_model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/visgator/projects/sgg/modeling/_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelInput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_text_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_detections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/visgator/deepsight/modeling/layers/clip/_vision.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBatched3DTensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatched3DTensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/visgator/deepsight/modeling/layers/clip/_vision.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         tmp = self.encoder(\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    652\u001b[0m                 )\n\u001b[1;32m    653\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m                 layer_outputs = encoder_layer(\n\u001b[0m\u001b[1;32m    655\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         hidden_states, attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    384\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1843\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1844\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1845\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.31 GiB (GPU 0; 14.75 GiB total capacity; 12.00 GiB already allocated; 22.81 MiB free; 13.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBuUL1cKKpG6"
      },
      "source": [
        "#### Results\n",
        "\n",
        "We trained **Scene Graph Grounder** on a single NVIDIA H100. Due to computational limitations and costs, SGG has been trained only once for 15 epochs, amounting to 35 hours (56h expected for 24 epochs). Hereafter, we report some plots to summarize the results of the training session (to see all the logged statistics see the run on [Weights & Biases](https://wandb.ai/visgator/sgg/runs/z3ruios2))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMN-t9_BKpG6"
      },
      "source": [
        "<img src=\"https://github.com/FrancescoGentile/visgator/blob/deepsight/docs/img/total_loss.png?raw=true\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQo7eP0rKpG6"
      },
      "source": [
        "<img src=\"https://github.com/FrancescoGentile/visgator/blob/deepsight/docs/img/giou.png?raw=true\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB40sx-nKpG6"
      },
      "source": [
        "<img src=\"https://github.com/FrancescoGentile/visgator/blob/deepsight/docs/img/iou.png?raw=true\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBIKR-f5KpG7"
      },
      "source": [
        "<img src=\"https://github.com/FrancescoGentile/visgator/blob/deepsight/docs/img/accuracy_50.png?raw=true\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHkU3p0lKpG7"
      },
      "source": [
        "<img src=\"https://github.com/FrancescoGentile/visgator/blob/deepsight/docs/img/accuracy_75.png?raw=true\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyOuL2PhKpG7"
      },
      "source": [
        "<img src=\"https://github.com/FrancescoGentile/visgator/blob/deepsight/docs/img/accuracy_90.png?raw=true\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "From the reported plots we can make two main observations:\n",
        "1. The performances of the model improves slowly. Indeed, both train and evaluation metrics improve approximately by 0.01 every epoch. This may be due to a too small learning rate that does not allow the model update its parameters fast enough. The second possibility is that the model already starts from a good initialization point (thanks to the position bias given by the detector) and has not enough capacity to significantly improve its performances. Thus, further works could focus also on scaling the architecture: our current model amounts up to **154,706,948 (155M)** trainable parameters; but only 3.8M parameters belong to the decoder (all the others are from CLIP). Thus, the decoder may not have enough capacity to refine the predictions made by OwlViT. Another possibility could be to use a better object detector like Grounding DINO. Finally, note that the limited capacity of the model may not necessarily be due to the decoder, but it may depend also on the vision encoder. Indeed, the CLIP vision encoder was trained on image level tasks thus it is not trained to extract region level features that are needed to perform object detection.\n",
        "2. The model is clearly overfitting after epoch 8. Indeed, while all training metrics keep improving, all evaluation metrics starts to decrease. This can be seen also by looking at the training and evaluation losses. All the training losses keep decreasing, while most of the evaluation losses starts to increase or remain stable after epoch 8. In particular, the GIoU loss for all layers has a huge increase after epoch 13, while the InfoNCE increases much more slowly. The only loss that keeps decreasing is the L1 loss. Unfortunately, we were not able to perform more experiments to try to solve this problem. Possible ways may consist in increasing the weight decay, adding data augmentations, increasing the dropout (in our experiment we used a relatively low dropout of 0.1) or adopting  dropout strategies like DropHead ([Zhou et al. 2020](https://arxiv.org/abs/2004.13342)) or structured dropout ([Fan et al. 2019](https://arxiv.org/1909.11556)), even though the number of layers should not be the cause of the overfitting since we only have 3 decoder layers.\n",
        "\n",
        "As for the data augmentations, in the repository we have already implemented various augmentations using `torchvision` and `albumentations` (see `deepsight.data.transformations`). We decided not to use them to first verify whether the model was able to learn in the simplest setting. However, we believe that data augmentations could be very useful to improve the performances of the model, since are often used in object detection models. However, due to the functioning of the proposed model, many traditionally used augmentation cannot be used. For example, random cropping cannot be used since it may lead to the removal of entities referred in the sentence. Similarly, geometric operations like random rotation, affine transformations or perspective transformations can be applied only on a small scale since they may alter the spatial position of an entity and thus compromise the spatial attributes contained in the input sentence. Lastly, also pixel transformation like hue or color jittering can not be applied too strongly since thay may cause complete alteration of the colours in the image and thus compromise the colour attributes contained in the input sentence (for example, if the sentence says \"the red car\", if the image is too much altered, the car may not be red anymore)."
      ],
      "metadata": {
        "id": "v7PKeFuVV7Xv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4rlxKF7KpG8"
      },
      "source": [
        "### Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the effectiveness of the proposed model, we define two different baselines."
      ],
      "metadata": {
        "id": "hwPHLY1aNehG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first baseline is the ones proposed in the assignment, that is we use YOLOv8 extra to detect a set of boundign boxes in the input image. Then we crop the image using the detected bounding boxes and we use CLIP to compute the similarity between the cropped images and the text embedding of the region description. The bounding box with the highest similarity is then returned as the candidate bounding box."
      ],
      "metadata": {
        "id": "GeBZPd5UNW21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import Tensor, nn\n",
        "from transformers.models.clip import CLIPModel, CLIPProcessor\n",
        "\n",
        "from deepsight.data.structs import Batch, BoundingBoxes, ODInput, RECInput, RECOutput\n",
        "from deepsight.modeling.detectors import YOLO\n",
        "from deepsight.modeling.pipeline import Model as _Model\n",
        "\n",
        "from projects.yoloclip.modeling import Config\n",
        "\n",
        "\n",
        "class Model(_Model[Batch[RECInput], Batch[RECOutput]]):\n",
        "    def __init__(self, config: Config) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self._dummy = nn.Parameter(torch.empty(0))\n",
        "\n",
        "        self.yolo = YOLO(config.yolo, config.box_threshold)\n",
        "\n",
        "        self.clip = CLIPModel.from_pretrained(config.clip.weights())\n",
        "        self.processor = CLIPProcessor.from_pretrained(config.clip.weights())\n",
        "\n",
        "    def forward(self, inputs: Batch[RECInput]) -> Batch[RECOutput]:\n",
        "        det_results = self.yolo(Batch([ODInput(inp.image, []) for inp in inputs]))\n",
        "\n",
        "        outputs = []\n",
        "        for sample_idx, result in enumerate(det_results):\n",
        "            if len(result.entities) == 0:\n",
        "                outputs.append(RECOutput(result.boxes))\n",
        "                continue\n",
        "\n",
        "            image = inputs[sample_idx].image.denormalize().data\n",
        "            cropped_regions: list[Tensor] = []\n",
        "            boxes = result.boxes.to_xyxy().denormalize()\n",
        "            for bbox in boxes.tensor:\n",
        "                x1, y1, x2, y2 = bbox.int()\n",
        "                cropped_regions.append(image[:, y1:y2, x1:x2])\n",
        "\n",
        "            tmp = self.processor(\n",
        "                text=inputs[sample_idx].description,\n",
        "                images=cropped_regions,\n",
        "                return_tensors=\"pt\",\n",
        "            ).to(self._dummy.device)\n",
        "\n",
        "            output = self.clip(**tmp)\n",
        "            idx = output.logits_per_image.argmax(0).item()\n",
        "\n",
        "            outputs.append(\n",
        "                RECOutput(\n",
        "                    BoundingBoxes(\n",
        "                        boxes.tensor[idx],\n",
        "                        images_size=boxes.images_size[idx],\n",
        "                        format=boxes.format,\n",
        "                        normalized=boxes.normalized,\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return Batch(outputs)"
      ],
      "metadata": {
        "id": "PDMbyBvTNmX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second baseline consists in simply giving in input to OwlViT the image and region description and then select the bouding box with the highest confidence score."
      ],
      "metadata": {
        "id": "Tj96uja4N2kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepsight.data.structs import Batch, ODInput, RECInput, RECOutput\n",
        "from deepsight.modeling.detectors import OwlViT\n",
        "from deepsight.modeling.pipeline import Model as _Model\n",
        "\n",
        "from projects.owlvit.modeling import Config\n",
        "\n",
        "\n",
        "class Model(_Model[Batch[RECInput], Batch[RECOutput]]):\n",
        "    def __init__(self, config: Config) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.detector = OwlViT(config.box_threshold, 1)\n",
        "\n",
        "    def forward(self, inputs: Batch[RECInput]) -> Batch[RECOutput]:\n",
        "        tmp = Batch([ODInput(inp.image, [inp.description]) for inp in inputs])\n",
        "        results = self.detector(tmp)\n",
        "\n",
        "        outputs = []\n",
        "        for result in results:\n",
        "            idx = result.scores.argmax()\n",
        "            box = result.boxes[idx]\n",
        "            outputs.append(RECOutput(box))\n",
        "\n",
        "        return Batch(outputs)\n"
      ],
      "metadata": {
        "id": "NPASM0qYNuWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the configurations used to test our model and the two baselines."
      ],
      "metadata": {
        "id": "wIwzDNLKtj2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the trained weights\n",
        "\n",
        "!gdown 1NsFEpRxwO8lI22K4dkvH_n9RXcls0pGO\n",
        "!mkdir weights\n",
        "!mv weights.pt weights/"
      ],
      "metadata": {
        "id": "maIAtuSiuWyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from datasets.refcocog import Config as RefCOCOGConfig\n",
        "from deepsight.engines.tester import Config\n",
        "from deepsight.utils.wandb import Config as WandbConfig\n",
        "\n",
        "sgg_test_config = Config(\n",
        "    dataset=RefCOCOGConfig(Path(\"data/refcocog\")),\n",
        "    pipeline=pipeline_config,\n",
        "    wandb=WandbConfig(\n",
        "        enabled=False,\n",
        "        job_type=\"test\",\n",
        "        project=\"tests\",\n",
        "        entity=\"visgator\",\n",
        "        save=False,\n",
        "    ),\n",
        "    weights=Path(\"weights/weights.pt\"),\n",
        ")\n"
      ],
      "metadata": {
        "id": "7t8XhvnDs3ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from datasets.refcocog import Config as RefCOCOGConfig\n",
        "from deepsight.engines.tester import Config\n",
        "from deepsight.modeling.detectors import YOLOModel\n",
        "from deepsight.utils.wandb import Config as WandbConfig\n",
        "from projects.yoloclip.modeling import Config as PipelineConfig\n",
        "\n",
        "yoloclip_test_config = Config(\n",
        "    dataset=RefCOCOGConfig(Path(\"data/refcocog\")),\n",
        "    pipeline=PipelineConfig(yolo=YOLOModel.EXTRA),\n",
        "    wandb=WandbConfig(\n",
        "        enabled=False,\n",
        "        job_type=\"test\",\n",
        "        project=\"tests\",\n",
        "        entity=\"visgator\",\n",
        "        save=False,\n",
        "    ),\n",
        ")\n"
      ],
      "metadata": {
        "id": "7hOs9g-ktD5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from datasets.refcocog import Config as RefCOCOGConfig\n",
        "from deepsight.engines.tester import Config\n",
        "from deepsight.utils.wandb import Config as WandbConfig\n",
        "from projects.owlvit.modeling import Config as PipelineConfig\n",
        "\n",
        "owlvit_test_config = Config(\n",
        "    dataset=RefCOCOGConfig(Path(\"data/refcocog\")),\n",
        "    pipeline=PipelineConfig(),\n",
        "    wandb=WandbConfig(\n",
        "        enabled=False,\n",
        "        job_type=\"test\",\n",
        "        project=\"tests\",\n",
        "        entity=\"visgator\",\n",
        "        save=False,\n",
        "    ),\n",
        ")\n"
      ],
      "metadata": {
        "id": "7nTjeN-vtKqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To start the testing run:\n",
        "\n",
        "from typing import Any\n",
        "\n",
        "from deepsight.engines.tester import Tester\n",
        "\n",
        "tester: Tester[Any, Any] = Tester.new(sgg_test_config) # substitute the passed config with the one of the model to test\n",
        "tester.run()"
      ],
      "metadata": {
        "id": "PKyFQP39tUFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare the three models we use the same metrics used in the training phase, that is:\n",
        "1. Intersection over Union (IoU);\n",
        "2. Generalized IoU (GIoU);\n",
        "3. Accuracy@50 counts how many predictions have a IoU greater than 0.5;\n",
        "4. Accuracy@75 counts how many predictions have a IoU greater than 0.75;\n",
        "5. Accuracy@90 counts how many predictions have a IoU greater than 0.9.\n",
        "\n",
        "Hereafter, we report the results (these results can also be seen in [Weight & Biases](https://wandb.ai/visgator/tests)):"
      ],
      "metadata": {
        "id": "IqkF1M-4L6hG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|Method|Accuracy@50|Accuracy@75|Accuracy@90|GIoU|IoU|\n",
        "|---|---|---|---|---|---|\n",
        "|SGG|<ins>65.18</ins>|<ins>54.36</ins>|29.33|<ins>0.5293</ins>|<ins>0.5998</ins>|\n",
        "|YOLOCLip|56.05|52.78|<ins>45.87<ins>|0.4720|0.5661|\n",
        "|OwlViT|48.55|38.87|21.20|0.3777|0.4718|"
      ],
      "metadata": {
        "id": "9rwEPKL4LJZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also report the results of current or past state of the art models. In particular, for each model we report the datasets on which it was trained, whether it was finetuned on RefCOCOg and its Accuracy@50.\n",
        "\n",
        "As we can see, our model performs worse than all these models. This however should not come as surprise since such models are trained on exponentially more data than our model and they have many more weights. Interesting is the fact that both GLIP and OwlViT, the only models not finetuned on RefCOCOg, perform much worse than the other models. This may indicate that visual grounding requires specific supervision."
      ],
      "metadata": {
        "id": "jJeNi-vaeTGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|Model|Pre-Training data|Finetuned|Accuracy@50|\n",
        "|---|---|---|---|\n",
        "|RefTR ([Li et al. 2022](https://arxiv.org/abs/2106.03089))| VG | yes | 80.01|\n",
        "|mDETR-ENB3 ([Kamath et al. 2021](https://arxiv.org/abs/2104.12763))|GoldG, RefC|yes|83.31|\n",
        "|DQ-DETR ([Liu et al. 2022](https://arxiv.org/abs/2211.15516)| GoldG, RefC | yes |83.44|\n",
        "|mPLUG-2 ([Xu et al. 2023](https://arxiv.org/abs/2302.00402))|COCO, VG, CC3M, CC12M, SBU|yes|85.14|\n",
        "|GLIP-T ([Li et al. 2021](https://arxiv.org/abs/2112.03857))|O365, GoldG, Cap4M|no|66.89|\n",
        "|Grounding-DINO-L ([Liu et al. 2023](https://arxiv.org/abs/2303.05499))|O365, OI, GoldG, Cap4M, COCO, RefC|yes|87.02|\n",
        "|OwlViT|O365, VG|no|48.55|\n",
        "|__Scene Graph Grounder__ (Ours)| None |yes|65.18|\n",
        "\n",
        "where O365 stands for Object365, OI for OpenImage, CC for Conceptual Captions, RefC for all three RefCOCO datasets, VG for Visual Genome.\n"
      ],
      "metadata": {
        "id": "gIXE-yIYOUAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Future works\n",
        "\n",
        "We report here three interesting ways in which the model could be improved in the future:\n",
        "1. The first way is to use a better vision encoder. As we have seen, the CLIP vision encoder is not trained to extract region level features that are needed to perform object detection. Thus, it would be interesting to use a vision encoder that is trained to extract region level features.\n",
        "2. Remove the detector and make the decoder perform the detection task from scratch. First of all, this would reduce the training and inference time since the model would be much smaller. Furthermore, this would require the model to be trained on larger dataset (like LVIS, Object365, etc.) and thus the modle would be able to learn better features. Particularly interesting would be training the model on Visual Genome since it already provides the scene graph for each region. This would allow to train the model on higher quality data and to supervise the model not only using the bounding box of the subject but the boundign boxes of all the entities referred in the region description.\n",
        "3. Train a new text encoder that would not only encode the text but it would also build the scene graph of the inout sentence (similarly to how [Attardi et al. 2022](https://github.com/Unipisa/diaparser) uses a transformer to obtain the dependency graph of the input sentence). This would allow not only to generate more accurate scene graphs but it would also lead the model to generate text encodings that contain high-level information about the context described in the sentence."
      ],
      "metadata": {
        "id": "kcyCgDTTLN6F"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "AO3ByYe8KpGs"
      ]
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "86f4401400f24e558c12c0838dacf0a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce0991cd9f924c18a0ccd2ee3dffc918",
              "IPY_MODEL_c84b4bb7566f4ebca2d02a24ce68900f",
              "IPY_MODEL_c1903c33b24a46df8abdc84ece029b46"
            ],
            "layout": "IPY_MODEL_b12a6bd898314bcaa8509a114a6a2fda"
          }
        },
        "ce0991cd9f924c18a0ccd2ee3dffc918": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96d910be16694d37b7f2d9f694ce9482",
            "placeholder": "​",
            "style": "IPY_MODEL_67336c6b893342f9afad1cb8d43fc744",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "c84b4bb7566f4ebca2d02a24ce68900f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_937a571c99384c4098f8ba42dd8f0ac7",
            "max": 666,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ebb2dec18d44624aa98ebc4d0d3d9c8",
            "value": 666
          }
        },
        "c1903c33b24a46df8abdc84ece029b46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21ab4c465f814b32af577748989bfe7a",
            "placeholder": "​",
            "style": "IPY_MODEL_e7ab3e218be642689e3eddde0a4b6d40",
            "value": " 666/666 [00:00&lt;00:00, 24.9kB/s]"
          }
        },
        "b12a6bd898314bcaa8509a114a6a2fda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96d910be16694d37b7f2d9f694ce9482": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67336c6b893342f9afad1cb8d43fc744": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "937a571c99384c4098f8ba42dd8f0ac7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ebb2dec18d44624aa98ebc4d0d3d9c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "21ab4c465f814b32af577748989bfe7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7ab3e218be642689e3eddde0a4b6d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90bdbb4ac7f64987b474625440df8385": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4ed0dd2961540c190adb2c78f65b1a3",
              "IPY_MODEL_0f5f82f31f714bc0b2da4382e38414dc",
              "IPY_MODEL_46f970d47fa549189ae3d4e528f2e498"
            ],
            "layout": "IPY_MODEL_f453ef1662b247ea84725a6c610d2d8a"
          }
        },
        "e4ed0dd2961540c190adb2c78f65b1a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b9332538dff406cac3f0a4603bfd0b5",
            "placeholder": "​",
            "style": "IPY_MODEL_c7815161ca3f4551a9db00e3852956e2",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "0f5f82f31f714bc0b2da4382e38414dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e0f0ebb09a84b30a63f06483e0a9033",
            "max": 440343552,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_288cc12fd6404bdeb6edf98123d0ec05",
            "value": 440343552
          }
        },
        "46f970d47fa549189ae3d4e528f2e498": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_895f1778658640e5859d8b557ab1becc",
            "placeholder": "​",
            "style": "IPY_MODEL_a9966204462b4c82ad99933db6e53714",
            "value": " 440M/440M [00:01&lt;00:00, 244MB/s]"
          }
        },
        "f453ef1662b247ea84725a6c610d2d8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b9332538dff406cac3f0a4603bfd0b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7815161ca3f4551a9db00e3852956e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e0f0ebb09a84b30a63f06483e0a9033": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "288cc12fd6404bdeb6edf98123d0ec05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "895f1778658640e5859d8b557ab1becc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9966204462b4c82ad99933db6e53714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78566cfde03f479985ccdb89d41b832b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1c22003fcf7b4e3aad677ba281619a7c",
              "IPY_MODEL_9d2214bad7054a288d8b0badf54cf1a7",
              "IPY_MODEL_ca9337525a1b4b04a369717d3af4c4e5"
            ],
            "layout": "IPY_MODEL_a36c5cfea94e4f1c9fceceee902d341c"
          }
        },
        "1c22003fcf7b4e3aad677ba281619a7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a8d3ba5929d47439a59796ab4fc86d0",
            "placeholder": "​",
            "style": "IPY_MODEL_eeaaa228597b40e988d31afa8f99d365",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "9d2214bad7054a288d8b0badf54cf1a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69ff9de76ea34cedbbc09d8f28838cfe",
            "max": 27,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_adc5d768b80b4d38905505c0d12e3ef1",
            "value": 27
          }
        },
        "ca9337525a1b4b04a369717d3af4c4e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0f9c385be0a44c3937bdaf52bb16ad8",
            "placeholder": "​",
            "style": "IPY_MODEL_9ba897e6ef3f494686b4461eac2b25eb",
            "value": " 27.0/27.0 [00:00&lt;00:00, 2.10kB/s]"
          }
        },
        "a36c5cfea94e4f1c9fceceee902d341c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a8d3ba5929d47439a59796ab4fc86d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeaaa228597b40e988d31afa8f99d365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69ff9de76ea34cedbbc09d8f28838cfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adc5d768b80b4d38905505c0d12e3ef1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0f9c385be0a44c3937bdaf52bb16ad8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ba897e6ef3f494686b4461eac2b25eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1ea8b9202fc47458987a1040396a0b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e02ba7ab66143d2b143e2e8ece32456",
              "IPY_MODEL_7b26b558b2c248c096aca6c655a6c31d",
              "IPY_MODEL_4911ca1568f442cb95583d8c274486c2"
            ],
            "layout": "IPY_MODEL_debc059cf70740d985d33399f725ec9b"
          }
        },
        "6e02ba7ab66143d2b143e2e8ece32456": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3efe0417484341a38fecdbd657222cc3",
            "placeholder": "​",
            "style": "IPY_MODEL_dee37c7187c34689bf9df559f2f6a40f",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "7b26b558b2c248c096aca6c655a6c31d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7772094ba3d6405581042233a20b4d28",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7bbb9cc9dc694d8bad724f49074f4037",
            "value": 231508
          }
        },
        "4911ca1568f442cb95583d8c274486c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bac244b54e9c4dbdb70cdace8f049eeb",
            "placeholder": "​",
            "style": "IPY_MODEL_152a1b173ca1424abebb5484a3327fee",
            "value": " 232k/232k [00:00&lt;00:00, 512kB/s]"
          }
        },
        "debc059cf70740d985d33399f725ec9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3efe0417484341a38fecdbd657222cc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dee37c7187c34689bf9df559f2f6a40f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7772094ba3d6405581042233a20b4d28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bbb9cc9dc694d8bad724f49074f4037": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bac244b54e9c4dbdb70cdace8f049eeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "152a1b173ca1424abebb5484a3327fee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1963cfda021540a0bf407f49671da25f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f94a3ccef30b4b93b7fc4825713cb1d1",
              "IPY_MODEL_b1fbede332db4b6e90def84ff70903a1",
              "IPY_MODEL_27414f0c0c884b6abf1c4ea4f5e11d1b"
            ],
            "layout": "IPY_MODEL_728a5a956ae448d0aef8efad986689b2"
          }
        },
        "f94a3ccef30b4b93b7fc4825713cb1d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36804f47d197479ea892d282faaa28cf",
            "placeholder": "​",
            "style": "IPY_MODEL_27fbf01336474e1c9c8a53faea1ffdf2",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "b1fbede332db4b6e90def84ff70903a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b5d8077a07646d2ba8d5f3a34a51e78",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e13c07b568e841b7b99accf83a2b77c9",
            "value": 466062
          }
        },
        "27414f0c0c884b6abf1c4ea4f5e11d1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f86d2a198e3e4907a08fab52aa1514df",
            "placeholder": "​",
            "style": "IPY_MODEL_82f1567473a049779bfa0dcea54481b8",
            "value": " 466k/466k [00:00&lt;00:00, 2.05MB/s]"
          }
        },
        "728a5a956ae448d0aef8efad986689b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36804f47d197479ea892d282faaa28cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27fbf01336474e1c9c8a53faea1ffdf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b5d8077a07646d2ba8d5f3a34a51e78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e13c07b568e841b7b99accf83a2b77c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f86d2a198e3e4907a08fab52aa1514df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82f1567473a049779bfa0dcea54481b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}